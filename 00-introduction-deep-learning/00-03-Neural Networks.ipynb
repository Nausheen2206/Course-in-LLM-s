{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a39e73",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "In the earlier chapters, we discussed the motivation behind deep learning and the growing need\n",
    "for models that can automatically learn useful representations from data. Traditional machine\n",
    "learning methods often rely on carefully engineered features, a process that is both time‑\n",
    "consuming and highly domain‑dependent. Neural networks address this limitation by learning\n",
    "representations directly from raw data in a data‑driven manner.\n",
    "\n",
    "In this chapter, we study **neural networks**, the fundamental computational structures that\n",
    "enable deep learning. Our goal here is not to immediately dive into algorithms or code, but to\n",
    "develop a clear conceptual understanding of how neural networks are constructed and why they\n",
    "are capable of modeling complex phenomena.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a48cd4",
   "metadata": {},
   "source": [
    "### The Artificial Neuron\n",
    "\n",
    "The basic unit of a neural network is the **artificial neuron**. Conceptually, an artificial\n",
    "neuron is designed to mimic the information‑processing behavior of a biological neuron. It\n",
    "receives multiple input signals, processes them, and produces a single output signal.\n",
    "\n",
    "Each input is associated with a **weight**, which represents the strength or importance of that\n",
    "input. The neuron computes a weighted sum of its inputs and adds a **bias** term. The bias allows\n",
    "the model to shift the output independently of the input values, thereby improving flexibility.\n",
    "\n",
    "Mathematically, the operation performed by a neuron can be written as:\n",
    "\n",
    "\\[\n",
    "z=w1​x1​+w2​x2​+⋯+wn​xn​+b\\]\n",
    "\n",
    "The value \\( z \\) is often referred to as the *pre‑activation* or *net input* of the neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7b41c",
   "metadata": {},
   "source": [
    "### Interpretation of the Neuron Computation\n",
    "\n",
    "From a geometric perspective, the weighted sum computed by a neuron defines a linear decision\n",
    "boundary in the input space. The weights determine the orientation of this boundary, while the\n",
    "bias controls its position.\n",
    "\n",
    "If the neuron outputs the value \\( z \\) directly, its behavior is equivalent to that of a\n",
    "linear regression or linear classification model. While such models are useful in simple\n",
    "settings, they are fundamentally limited in their ability to represent complex patterns found\n",
    "in real‑world data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e975b65",
   "metadata": {},
   "source": [
    "### Limitations of Linear Models\n",
    "\n",
    "Many practical problems involve relationships that cannot be captured using linear functions.\n",
    "For example, tasks such as image recognition, speech processing, and natural language\n",
    "understanding exhibit intricate non‑linear dependencies among input variables.\n",
    "\n",
    "One might expect that stacking multiple linear neurons into several layers would overcome this\n",
    "issue. However, the composition of linear functions is itself linear. As a result, a network\n",
    "composed solely of linear transformations—regardless of its depth—remains a linear model.\n",
    "\n",
    "This observation highlights a critical limitation and motivates the introduction of\n",
    "non‑linearity into neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a1601",
   "metadata": {},
   "source": [
    "### Activation Functions and Non‑Linearity\n",
    "\n",
    "To enable neural networks to model non‑linear relationships, the output of each neuron is passed\n",
    "through a **non‑linear activation function**. This function transforms the pre‑activation value\n",
    "\\( z \\) into an output that is then forwarded to the next layer.\n",
    "\n",
    "Activation functions play a central role in determining the expressive power of a neural\n",
    "network. By introducing non‑linearity at each layer, they allow deep networks to approximate\n",
    "highly complex functions that would otherwise be impossible to represent.\n",
    "\n",
    "At this stage, it is sufficient to understand activation functions as mechanisms that regulate\n",
    "information flow and introduce non‑linearity. Their specific mathematical forms and practical\n",
    "trade‑offs will be examined in detail in a subsequent chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4f6fe",
   "metadata": {},
   "source": [
    "### From Single Neurons to Neural Networks\n",
    "\n",
    "A **neural network** is formed by organizing neurons into layers. The first layer receives the\n",
    "input data, intermediate layers—known as **hidden layers**—perform successive transformations,\n",
    "and the final layer produces the network’s output.\n",
    "\n",
    "Information flows forward through the network, layer by layer, in what is known as the\n",
    "*forward pass*. During training, the network adjusts its weights and biases so that its outputs\n",
    "closely match the desired targets. This learning process is driven by optimization techniques\n",
    "that will be discussed later.\n",
    "\n",
    "With this conceptual foundation, we are now prepared to explore tensor representations,\n",
    "activation functions in depth, and the mathematical operations that underpin neural network\n",
    "training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e91c65",
   "metadata": {},
   "source": [
    "\n",
    "## Biological Inspiration of Neural Networks\n",
    "\n",
    "Neural networks are inspired by the structure and functioning of the human brain.\n",
    "The brain consists of billions of interconnected neurons that communicate using electrical signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb389d",
   "metadata": {},
   "source": [
    "\n",
    "### Key Components of a Biological Neuron\n",
    "\n",
    "- **Dendrites**: receive signals from other neurons  \n",
    "- **Cell body (soma)**: processes incoming signals  \n",
    "- **Axon**: transmits signals to other neurons  \n",
    "\n",
    "Learning occurs by strengthening or weakening synaptic connections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c5c88",
   "metadata": {},
   "source": [
    "\n",
    "## Artificial Neuron Model\n",
    "\n",
    "An artificial neuron is a mathematical model designed to mimic\n",
    "the behavior of a biological neuron.\n",
    "\n",
    "\n",
    "## What is a neuron?\n",
    "\n",
    "An artificial neuron (also referred to as a perceptron) is a mathematical function.  \n",
    "It takes one or more inputs that are multiplied by values called **weights** and added together.  \n",
    "This value is then passed to a non-linear function, known as an **activation function**,  \n",
    "to become the neuron’s output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac414e1",
   "metadata": {},
   "source": [
    "\n",
    "### Core Elements of an Artificial Neuron\n",
    "\n",
    "- Inputs represent features  \n",
    "- Weights represent importance of features  \n",
    "- Bias shifts the activation threshold  \n",
    "- Output represents the neuron response  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ff4f7",
   "metadata": {},
   "source": [
    "\n",
    "## Structure of an Artificial Neuron\n",
    "\n",
    "An artificial neuron computes its output in two stages:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cace9b",
   "metadata": {},
   "source": [
    "\n",
    "- **Linear combination** of inputs  \n",
    "- **Non-linear activation** of the result  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50785744",
   "metadata": {},
   "source": [
    "It begins with inputs. These inputs are nothing more than numbers, but each number carries meaning. In an image-processing task, they might represent pixel intensities; in a weather model, they could be temperature or humidity values. The neuron doesn’t understand what these numbers mean in the real world—it only knows their magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fae5d3",
   "metadata": {},
   "source": [
    "\n",
    "The computation can be written as:\n",
    "\n",
    "z = w₁x₁ + w₂x₂ + … + wₙxₙ + b  \n",
    "a = f(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304eb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "w = np.array([0.3, 0.5, -0.2])\n",
    "b = 0.4\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13994dc5",
   "metadata": {},
   "source": [
    "\n",
    "## Weights and Bias: Role and Significance\n",
    "\n",
    "Weights are the neuron’s way of listening carefully. They decide which inputs deserve attention and which should be ignored, shaping what the neuron learns over time. As training continues, these weights change, slowly storing experience and knowledge. Bias, on the other hand, gives the neuron freedom. It allows the neuron to respond even when inputs are weak or to stay silent until the right moment. Together, weights and bias guide how a neuron thinks, reacts, and learns from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f14714",
   "metadata": {},
   "source": [
    "\n",
    "### Weights\n",
    "\n",
    "- Control contribution of each input  \n",
    "- Learned during training  \n",
    "- Determine feature importance  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a352d7",
   "metadata": {},
   "source": [
    "\n",
    "### Bias\n",
    "\n",
    "- Allows activation even when inputs are zero  \n",
    "- Shifts decision boundary  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75417237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.array([2.0, 3.0])\n",
    "w = np.array([0.5, 0.5])\n",
    "\n",
    "np.dot(w, x), np.dot(w, x) + 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb02cc",
   "metadata": {},
   "source": [
    "\n",
    "Non-linearity allows neural networks to learn complex patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3d9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = np.array([-2, -1, 0, 1, 2])\n",
    "relu = np.maximum(0, z)\n",
    "relu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed1b4d",
   "metadata": {},
   "source": [
    "\n",
    "## Single Neuron Model (Perceptron)\n",
    "\n",
    "The perceptron is the simplest form of an artificial neuron, designed to make a basic decision. It begins by receiving inputs, each carrying a piece of information. These inputs are weighted according to their importance and then combined into a single value. A bias is added to give the neuron flexibility in deciding when to respond. This combined signal is passed through an activation function, usually a step function, which decides whether the neuron should activate or remain silent. In this way, the perceptron acts like a yes-or-no decision maker, forming the foundation upon which more complex neural networks are built\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e8c8c",
   "metadata": {},
   "source": [
    "\n",
    "### Perceptron Decision Rule\n",
    "\n",
    "- Output = 1 if (w·x + b) ≥ 0  \n",
    "- Output = 0 otherwise  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9b633",
   "metadata": {},
   "source": [
    "\n",
    "### Limitation of Perceptron\n",
    "\n",
    "The perceptron can only solve linearly separable problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0014d3",
   "metadata": {},
   "source": [
    "\n",
    "## Linear Transformation in Neural Networks (Wx + b)\n",
    "\n",
    "Neural networks use matrix operations to compute outputs efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2735d1",
   "metadata": {},
   "source": [
    "\n",
    "The linear transformation is expressed as:\n",
    "\n",
    "z = Wx + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d7a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "W = np.array([[0.2, 0.4], [0.6, 0.8]])\n",
    "b = np.array([0.1, 0.2])\n",
    "\n",
    "np.dot(X, W) + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77baa8",
   "metadata": {},
   "source": [
    "\n",
    "## From Single Neuron to Fully Connected Layer\n",
    "\n",
    "A single neuron can make only a simple decision, based on a limited view of the input. But when many such neurons are brought together, something more powerful emerges. In a fully connected layer, every neuron receives inputs from all neurons in the previous layer. Each connection has its own weight, allowing every neuron to learn a unique perspective of the same input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c8b19",
   "metadata": {},
   "source": [
    "\n",
    "- Each neuron has its own weights and bias  \n",
    "- All neurons receive the same input  \n",
    "- Enables learning multiple features  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238f5554",
   "metadata": {},
   "source": [
    "\n",
    "## Layers in a Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347d322",
   "metadata": {},
   "source": [
    "\n",
    "### Input Layer\n",
    "\n",
    "- Receives raw input features  \n",
    "- No trainable parameters  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef64666",
   "metadata": {},
   "source": [
    "\n",
    "### Hidden Layer\n",
    "\n",
    "- Performs intermediate computations  \n",
    "- Learns feature representations  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25f9e0",
   "metadata": {},
   "source": [
    "\n",
    "### Output Layer\n",
    "\n",
    "- Produces final prediction  \n",
    "- Depends on the task type\n",
    "\n",
    "![Artificial Neuron Diagram](https://miro.medium.com/v2/resize:fit:750/format:webp/1*ToPT8jnb5mtnikmiB42hpQ.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a623d",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Pass: Flow of Information Through the Network\n",
    "\n",
    "The forward pass is the moment when a neural network puts its knowledge to work. Information enters the network through the input layer, where each value represents a feature of the data. This information then flows forward, layer by layer, without looking back. At each neuron, inputs are weighted, combined, adjusted by a bias, and passed through an activation function. The resulting outputs become inputs for the next layer, gradually transforming raw data into meaningful patterns. By the time the signal reaches the output layer, the network produces its final prediction—this entire smooth journey of information is known as the forward pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd17f82",
   "metadata": {},
   "source": [
    "\n",
    "## Task for the reader\n",
    "\n",
    "1. Compute neuron output for different weights and bias  \n",
    "2. Compare ReLU and Sigmoid activation functions  \n",
    "3. Explain why non-linearity is necessary  \n",
    "4. Identify limitations of a single neuron  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
