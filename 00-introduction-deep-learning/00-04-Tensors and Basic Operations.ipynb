{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea984eda-a0f0-4755-852a-26a8607f9fdf",
   "metadata": {},
   "source": [
    "# Tensors and Basic Operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e9393-b76a-4bb2-b791-e45a864ded3e",
   "metadata": {},
   "source": [
    "## Tensors \n",
    "A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes at tf.dtypes.\n",
    "\n",
    "All tensors are immutable like Python numbers and strings: you can never update the contents of a tensor, only create a new one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a9760-6409-4b3f-82a6-7908c8b21d66",
   "metadata": {},
   "source": [
    "### Framework Used\n",
    "\n",
    "**Note-** This notebook uses **PyTorch** for implementing tensor operations and deep learning concepts. PyTorch is chosen due to its simplicity, dynamic computation graph, and ease of debugging, which makes it suitable for learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe32489-0f65-4512-aa1c-b9e075e2de47",
   "metadata": {},
   "source": [
    "**Data Types Include**: float32, int32, string and others.\n",
    "\n",
    "**Shape**: Represents the dimension of data.\n",
    "\n",
    "Just like vectors and matrices tensors can have operations applied to them like addition, subtraction, dot product, cross product etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52c0ef-0f5c-4c0a-9bab-d8942246dcc8",
   "metadata": {},
   "source": [
    "### Scalar\n",
    "- A tensor with **zero dimensions**\n",
    "- Represents a single numerical value\n",
    "- A scalar contains a single value, and no \"axes\".\n",
    "\n",
    "Example:  \n",
    "x = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a536c849-bd22-4593-a3f7-b9e118ba31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "scalar = torch.tensor(5)\n",
    "scalar\n",
    "x=scalar.shape\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242a397-1a8f-4280-be10-759cf16e9408",
   "metadata": {},
   "source": [
    "### Vector\n",
    "- A tensor with **one dimension**\n",
    "- Represents a list of numbers\n",
    "-  A vector has one axis\n",
    "\n",
    "Example:  \n",
    "x = [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dce20c-4e09-4efb-b1b4-753323dfae4d",
   "metadata": {},
   "source": [
    "### Matrix\n",
    "- A tensor with **two dimensions**\n",
    "- Represents rows and columns\n",
    "\n",
    "Example:  \n",
    "x = [[1, 2],  \n",
    "     [3, 4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61f49dc-c230-49c4-8976-75ae82bf8cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "matrix\n",
    "x=matrix.shape\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565efc27-7d84-41db-b979-041158ed115a",
   "metadata": {},
   "source": [
    "### Higher-Order Tensors\n",
    "- Tensors with **three or more dimensions**\n",
    "- Used to represent images, videos, and batches of data\n",
    "\n",
    "Example:\n",
    "- 3D tensor → RGB image\n",
    "- 4D tensor → batch of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c05a923-dc4a-4421-b2dc-7344fcf63305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_3d = torch.randn(2, 3, 4)\n",
    "tensor_3d.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d46b05-edd5-4f46-8c82-4584f2af1b44",
   "metadata": {},
   "source": [
    "### 1. Creating a Tensor from Python Data\n",
    "\n",
    "Tensors can be created directly from Python lists or nested lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238b2484-7ce4-4211-9784-abfea34b9da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10),\n",
       " tensor([1, 2, 3, 4]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar\n",
    "scalar = torch.tensor(10)\n",
    "\n",
    "# Vector\n",
    "vector = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# Matrix\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "scalar, vector, matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1243f4-7d37-4950-b49a-a730bf7c2d8b",
   "metadata": {},
   "source": [
    "### 2. Creating Tensors with Specific Values\n",
    "\n",
    "Deep learning frameworks provide functions to create tensors filled with predefined values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b334ec-7c0f-4ae8-8029-743e5c3fd600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor filled with zeros\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "\n",
    "# Tensor filled with ones\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "\n",
    "# Identity matrix\n",
    "identity_tensor = torch.eye(3)\n",
    "\n",
    "zeros_tensor, ones_tensor, identity_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9d6f5-2253-47d1-b56f-5d0e1853075c",
   "metadata": {},
   "source": [
    "### 3. Creating Random Tensors\n",
    "\n",
    "Random tensors are widely used to initialize weights in neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a130067-fbe7-4bfe-a478-39f392a7d2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9042, 0.0924, 0.8552],\n",
       "         [0.7995, 0.1496, 0.0989]]),\n",
       " tensor([[-0.6224, -0.5790,  1.2360],\n",
       "         [ 1.8613,  0.0829, -1.0737]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random values between 0 and 1\n",
    "random_uniform = torch.rand(2, 3)\n",
    "\n",
    "# Random values from normal distribution (mean=0, std=1)\n",
    "random_normal = torch.randn(2, 3)\n",
    "\n",
    "random_uniform, random_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da04ba0-34bf-4c91-82ea-b2e119bb59f1",
   "metadata": {},
   "source": [
    "### 4. Creating Tensors with a Specific Data Type\n",
    "\n",
    "Tensors can be created with specific data types such as integers or floating-point values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abeb5807-9bd0-4a05-a038-4ef8dd0e9448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "\n",
    "float_tensor.dtype, int_tensor.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e466e-3d76-4df8-8922-92c348d0d322",
   "metadata": {},
   "source": [
    "### 5. Creating Tensors with a Specific Shape\n",
    "\n",
    "Sometimes we need tensors of a particular shape for neural network layers.,\n",
    "(Shape: The length (number of elements) of each of the axes of a tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b659bc8a-eb2a-4b52-9bb2-63102db08232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "rank_4_tensor = torch.zeros(3, 2, 4, 5)\n",
    "print(rank_4_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6c9db-696d-4828-8263-a2a45b036d17",
   "metadata": {},
   "source": [
    "## Understanding the output \n",
    "Each level of nested square brackets corresponds to one axis of the tensor.\n",
    "\n",
    "The outermost level contains 3 large blocks, which represent the first axis. Inside each block, there are 2 sub-blocks, corresponding to the second axis. Each sub-block is a matrix with 4 rows, representing the third axis. Each row contains 5 values, which form the fourth axis.\n",
    "\n",
    "All values in the tensor are 0. because the tensor was created using torch.zeros(), which initializes every element to zero.\n",
    "\n",
    "You can think of this tensor as:\n",
    "\n",
    "3 groups of data\n",
    "\n",
    "each group containing 2 matrices\n",
    "\n",
    "each matrix having 4 rows\n",
    "\n",
    "each row containing 5 values\n",
    "## Understanding a Rank-4 Tensor Using Axes\n",
    "\n",
    "![4-Axis Tensor](https://www.tensorflow.org/static/guide/images/tensor/4-axis_block.png)\n",
    "\n",
    "The image above visually represents a **rank-4 tensor**, which means the tensor has **four axes (dimensions)**. Each axis represents one level of data organization. Instead of thinking of a tensor as just numbers, it is helpful to think of it as data arranged across multiple directions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df1efb5-6463-4ab1-b3f3-d4564eadc9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty tensor (values are uninitialized)\n",
    "empty_tensor = torch.empty(3, 2)\n",
    "\n",
    "# Create a tensor with the same shape as another tensor\n",
    "reference = torch.ones(2, 2)\n",
    "same_shape = torch.zeros_like(reference)\n",
    "\n",
    "empty_tensor, same_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2c76c-b12f-4d8a-a584-ce37e771627f",
   "metadata": {},
   "source": [
    "### 6. Creating Tensors Using Ranges\n",
    "\n",
    "Range-based tensors are useful for indexing and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f6ee42-3a7c-405b-82a1-4948a04aec1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence of numbers\n",
    "range_tensor = torch.arange(0, 10)\n",
    "\n",
    "# Evenly spaced values\n",
    "linspace_tensor = torch.linspace(0, 1, steps=5)\n",
    "\n",
    "range_tensor, linspace_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c67fa-27fe-4860-9d19-82d452e69c0c",
   "metadata": {},
   "source": [
    "### 7. Creating Tensors That Track Gradients\n",
    "\n",
    "For training neural networks, tensors must track gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63db2bec-d35b-44bf-8c8a-789770138c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917351e-8a03-4658-a7af-ad7f84340a75",
   "metadata": {},
   "source": [
    "## Key Points to Remember\n",
    "\n",
    "- Tensors can be created from lists, predefined values, or random distributions\n",
    "- Shape, data type, and device are important properties\n",
    "- Random tensors are commonly used for weight initialization\n",
    "- Gradient tracking is essential for training neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff3724-bb61-4fe6-8178-6fb9b344b8a3",
   "metadata": {},
   "source": [
    "## Indexing and Slicing of Tensors\n",
    "\n",
    "Indexing and slicing are used to access or extract specific elements or parts of a tensor.  \n",
    "This is very important in deep learning for selecting features, batches, channels, and regions of data.\n",
    "\n",
    "### What is Indexing?\n",
    "\n",
    "Indexing means selecting **individual elements** of a tensor using their position (index).  \n",
    "Indexing in tensors starts from **0**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d11c8028-4dba-45cc-858f-8ce275c1925c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 1D tensor (vector)\n",
    "x = torch.tensor([10, 20, 30, 40, 50])\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f38dd-8ae7-4515-9739-34be1c37e20d",
   "metadata": {},
   "source": [
    "### 1. Indexing a 1D Tensor (Vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "418f4fbf-46d7-4fe6-8f58-361200826b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]   # first element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17917afa-2fa4-4330-b22d-201a35d3a318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]   # third element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c03eecf8-d246-4679-916e-9d42b694d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1]  # last element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b42a1-6234-4a39-af5d-e5cb6adc8d57",
   "metadata": {},
   "source": [
    "- Positive index → counts from the beginning  \n",
    "- Negative index → counts from the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadb232-a5b6-4a8f-b861-8879631f808c",
   "metadata": {},
   "source": [
    "### 2. Slicing a 1D Tensor\n",
    "\n",
    "Slicing extracts a **range of elements** from a tensor.\n",
    "Syntax: `start : end` (end index is excluded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7896f02d-2104-474c-9dd5-1cfa008f1b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 30, 40])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:4]   # elements from index 1 to 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a493d184-5dab-4115-8dc9-620ed95b5c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:3]    # first three elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "958e1968-6f1c-4b7f-a5fc-5161b92363fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 40, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2:]    # elements from index 2 to end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9528-247f-4142-8b3c-94e4cd353559",
   "metadata": {},
   "source": [
    "### Step Size in Slicing\n",
    "\n",
    "Syntax: `start : end : step`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe2086fa-e600-4c74-880a-c8c75441cc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 30, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[::2]   # every second element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aed8db50-5a37-46f4-8b56-b203d22b1881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 40, 30, 20, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([10, 20, 30, 40, 50])\n",
    "torch.flip(x, dims=[0])\n",
    " # reverse the tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac5591-0f7f-4b2c-a827-5a2642333a3a",
   "metadata": {},
   "source": [
    "### 3. Indexing a 2D Tensor (Matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "754e6c14-3c9a-4168-91fa-ab4193417f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2D tensor\n",
    "m = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71770c-9da6-4feb-8e3e-f1f9aa828e72",
   "metadata": {},
   "source": [
    "### Accessing Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f4fd260-35a6-4c03-bafa-c3aa7307575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0]     # first row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "beb64018-db30-4e37-89b6-e2095acba837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[1]     # second row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba33b8-335e-4b92-ad21-da17926b6845",
   "metadata": {},
   "source": [
    "### Accessing Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98c143ec-c2d8-4afe-96b6-34137d320112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 0]  # first column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47b6de3b-8d36-4b13-864d-8244434497a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 2]  # third column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be523d3-54c4-4c0c-b922-6ce51d162cf4",
   "metadata": {},
   "source": [
    "### 3. Slicing a 2D Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a67f7d87-ee8e-414c-bf2d-f9b83c12dda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0:2, 1:3]  # rows 0–1 and columns 1–2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "391acc65-edcb-4a3d-922a-d26a3c308e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 1:]     # all rows, columns from index 1 onward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae6320f5-feb7-469a-82e2-a802c6df6472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 1:]     # all rows, columns from index 1 onward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcdabe-d541-4fcc-bd65-efb7b079e06f",
   "metadata": {},
   "source": [
    "### 4. Indexing Higher-Dimensional Tensors\n",
    "\n",
    "Higher-order tensors are common in deep learning.\n",
    "Example: 3D tensor (batch, rows, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27f55752-f247-4a34-8e01-912c1232752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(2, 3, 4)\n",
    "t.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf5640-b42d-4bdd-9b26-f691d5a85171",
   "metadata": {},
   "source": [
    "### 5. Using Ellipsis (...)\n",
    "\n",
    "Ellipsis is used to represent missing dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b2591e9-900b-465a-a740-2bf500844bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3199,  0.2449,  0.2132],\n",
       "        [-2.2484,  0.3037, -0.0742]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1]   # all elements from last dimension index 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2d248-ac6f-4276-b7f4-80d987bb65f6",
   "metadata": {},
   "source": [
    "### 6. Boolean Indexing\n",
    "\n",
    "Boolean indexing selects elements based on conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74dc9184-7898-4809-85d1-ffce0795e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 20])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5, 10, 15, 20])\n",
    "x[x > 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ffe23-2198-4d71-ba0d-f465e8ae92b2",
   "metadata": {},
   "source": [
    "## Reshaping Tensors\n",
    "\n",
    "Reshaping means changing the **shape (dimensions)** of a tensor **without changing its data**.  \n",
    "In deep learning, reshaping is very common when preparing data for neural network layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7482b-07fa-4343-b53e-84ea48649265",
   "metadata": {},
   "source": [
    "### Why Reshaping is Needed in Deep Learning\n",
    "\n",
    "- Neural network layers expect inputs in specific shapes  \n",
    "- Images, text, and batches must be reshaped correctly  \n",
    "- Helps convert data between layers (e.g., CNN → Fully Connected layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bcf91-1bef-4252-b871-9be3b3a56e8c",
   "metadata": {},
   "source": [
    "### Understanding Tensor Shape\n",
    "\n",
    "The **shape** of a tensor tells:\n",
    "- How many dimensions it has  \n",
    "- How many elements are in each dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "539232cc-5157-440d-9be3-1ef40857c640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0801c2-c771-45e2-a3ec-28d38bfa5245",
   "metadata": {},
   "source": [
    "### 1. Reshaping Using `reshape()`\n",
    "\n",
    "The `reshape()` function changes the shape of a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd1ca4fc-d36a-4645-99e3-133e691236ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "x_reshaped = x.reshape(2, 3)\n",
    "x_reshaped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19e20c-5066-4952-b027-b44a46f2bf04",
   "metadata": {},
   "source": [
    "Rule:  \n",
    "Total number of elements **must remain the same**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a89d87-2667-42c3-9bd0-d6546f8ff839",
   "metadata": {},
   "source": [
    "### 2. Using `-1` to Automatically Infer Size\n",
    "\n",
    "PyTorch can automatically calculate one dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8eb4209-1310-4a70-8793-ba3c841118da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d95243-f0a1-4bfd-8c4f-5d0d439855f8",
   "metadata": {},
   "source": [
    "Here, PyTorch finds the correct value for `-1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d41e8-4f78-44b2-8071-9875d390871b",
   "metadata": {},
   "source": [
    "### 3. Flattening a Tensor\n",
    "\n",
    "Flattening means converting a multi-dimensional tensor into a 1D tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5791da29-325a-428a-9b92-7bfcb8db00fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "m.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7b5da-232b-4cbc-8b98-59eb67f84646",
   "metadata": {},
   "source": [
    "### 4. `view()` vs `reshape()`\n",
    "\n",
    "Both change the shape of a tensor, but they differ internally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e324dbc-84a0-44f6-bce6-ebca37a38833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "x.view(2, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f490b-c919-4bd7-831c-23e7eb8a7032",
   "metadata": {},
   "source": [
    "- `view()` works only if tensor memory is contiguous  \n",
    "- `reshape()` is safer and preferred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a087d24-c3b0-4e9f-9a88-260f60017471",
   "metadata": {},
   "source": [
    "### 5. Adding or Removing Dimensions\n",
    "\n",
    "Sometimes we need to add or remove dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "242a56b5-e13a-4595-9698-f426a1cbd464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension\n",
    "x.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70f1f0ec-a2de-4500-8056-020cd629a060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimensions of size 1\n",
    "x.unsqueeze(0).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f520e5-325f-41df-bb87-f9c9c1f6b262",
   "metadata": {},
   "source": [
    "## Tensor Arithmetic Operations\n",
    "\n",
    "Tensor arithmetic operations are mathematical operations performed on tensors.  \n",
    "These operations are applied **element-wise** and are heavily used in deep learning for computing outputs, losses, and gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f82b90d6-cb8b-4449-b9ce-9c0cb4e0dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e3e50a1-6af6-4374-89f2-f4f5d1c50c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addition\n",
    "a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc9504b7-8a92-436b-a717-ec36e12d8caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3, -3, -3])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtraction\n",
    "a - b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66fdc5cc-f9d9-4ca4-967b-928dc01b2a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10, 18])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplication (element-wise)\n",
    "a * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37d24e6b-b195-4e30-b364-f13f2d97354c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Division\n",
    "a / b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0634fc-de18-4def-ab77-895e98c2c79f",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8e1e40e-b989-4466-ad1b-a6fc573d3d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19, 22],\n",
       "        [43, 50]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.tensor([[1, 2],\n",
    "                   [3, 4]])\n",
    "m2 = torch.tensor([[5, 6],\n",
    "                   [7, 8]])\n",
    "\n",
    "torch.matmul(m1, m2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59bd6f-1d21-4020-aa82-9f93b7df67dd",
   "metadata": {},
   "source": [
    "## Broadcasting \n",
    "\n",
    "Broadcasting allows arithmetic operations between tensors of **different shapes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205aa1b0-43f3-4318-a989-72e4714e487b",
   "metadata": {},
   "source": [
    "### Rules of Broadcasting\n",
    "1. Compare tensor shapes from right to left  \n",
    "2. Dimensions are compatible if they are equal or one of them is 1  \n",
    "3. Smaller tensor is automatically expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99f4ed39-0392-4bee-81a7-3a644cd944a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 6],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "y = torch.tensor([1, 2, 3])\n",
    "\n",
    "x + y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a709c24-aab4-491a-9150-9f034216ba41",
   "metadata": {},
   "source": [
    "Here, `y` is broadcast across rows of `x`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959dd19-5f4f-4f02-9adc-75234051a736",
   "metadata": {},
   "source": [
    "Broadcasting avoids copying data and improves performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb600fcd-13ed-4411-be17-85a2b52e5981",
   "metadata": {},
   "source": [
    "The output shows the **probabilities** of each class.  \n",
    "Softmax is ideal for problems with >2 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07043025-462e-4798-8954-ee060813c62c",
   "metadata": {},
   "source": [
    "## Key Concepts of Automatic Differentiation in Tensorflow\n",
    "### 1.Computatinal Graph\n",
    "\n",
    "Computational graphs are used to represent mathematical expressions in a structured way. In deep learning, they act like a descriptive language that explains how a model performs its computations. Formally, a computational graph is a directed graph that shows how values are computed and how data flows through different operations.\n",
    "\n",
    "A computational graph supports two kinds of calculations: forward computation and backward computation. During forward computation, input values move through the graph and each operation produces an output, eventually giving the final result. During backward computation, gradients are calculated by moving backward through the graph, which is essential for training neural networks.\n",
    "\n",
    "In a computational graph, variables are represented as nodes. These variables can be scalars, vectors, matrices, tensors, or other data structures. Edges represent data dependencies or function arguments, showing how one variable influences another. Operations such as addition, subtraction, or multiplication are also represented in the graph, and more complex functions are built by combining these simple operations.\n",
    "\n",
    "For example, consider the expression\n",
    "\n",
    "\\[\n",
    "Y = (a + b) * (b - c)\n",
    "\\]\n",
    "\n",
    "To make the computation clearer, we introduce intermediate variables:\n",
    "\n",
    "\\[\n",
    "d = a + b\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "e = b - c\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "Y = d * e\n",
    "\\]\n",
    "\n",
    "Each of these steps becomes a node in the computational graph, and the arrows show how outputs from one operation are used as inputs for the next.\n",
    "\n",
    "\n",
    "Each of these steps becomes a node in the computational graph, and the arrows show how outputs from one operation are used as inputs for the next.\n",
    "\n",
    "In deep learning, computational graphs explain why training is divided into two phases. The forward pass computes the output and loss of the neural network, while the backward pass computes gradients using the chain rule. These gradients tell us how changes in inputs or parameters affect the final output.\n",
    "\n",
    "When computing derivatives, the key idea is understanding how a small change in one variable affects another variable that depends on it. If a variable directly or indirectly influences the output, its contribution is captured using partial derivatives. By applying the chain rule across the graph, backpropagation efficiently computes derivatives with respect to all input variables.\n",
    "\n",
    "There are two main types of computational graphs. \n",
    "\n",
    "**Static computational graphs** are defined completely before execution, which allows strong optimization and faster performance but makes them less flexible for variable-sized or dynamic data.\n",
    "\n",
    "**Dynamic computational graphs** are built during execution, making them easier to debug and more flexible, though they offer fewer opportunities for global optimization.\n",
    "\n",
    "![Computational Graph](https://media.geeksforgeeks.org/wp-content/uploads/20200527151747/e19.png)\n",
    "\n",
    "![Computational Graph1](https://media.geeksforgeeks.org/wp-content/uploads/20200527173723/e34.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce5b5b-00a3-4989-ba47-cb29e0ead5f6",
   "metadata": {},
   "source": [
    "### 2.Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a powerful machine learning technique that builds a strong model by combining many weak models. The main idea is simple: instead of trying to build one perfect model, we build models step by step, where each new model focuses on correcting the mistakes made by the previous ones.\n",
    "\n",
    "In Gradient Boosting, models are added sequentially, not independently. Each new model is trained to reduce the error/loss of the overall system. This is why it is called boosting ,every new model boosts the performance of the existing ensemble.\n",
    "\n",
    "The process begins with a simple model, often one that makes very rough predictions. We then calculate how wrong these predictions are using a loss function. The next model is trained to predict these errors. This process is repeated many times, gradually improving the model’s predictions.\n",
    "\n",
    "The term *gradient* comes from optimization. At each step, Gradient Boosting uses the gradient of the loss function to decide how the next model should improve the predictions. In other words, the model moves in the direction that most reduces the error.\n",
    "\n",
    "Decision trees are commonly used as the weak learners in Gradient Boosting. These trees are usually shallow, meaning they are simple and make limited decisions. Even though each tree is weak on its own, combining many of them results in a very strong predictive model.\n",
    "\n",
    "Gradient Boosting works well for both regression and classification problems. It is widely used because it can capture complex patterns in data and often delivers high accuracy. However, it can be sensitive to noise and overfitting if not properly tuned.\n",
    "\n",
    "Important parameters in Gradient Boosting include the number of trees, learning rate, and tree depth. The learning rate controls how much each new model contributes, while the number of trees determines how long the boosting process continues.\n",
    "\n",
    "Because Gradient Boosting builds models sequentially, training can be slower compared to methods like bagging. However, the performance gains often justify the additional computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc06b4-4338-4d01-b6cf-ebf5e407f23c",
   "metadata": {},
   "source": [
    "### Steps Involved in Gradient Boosting\n",
    "\n",
    "Gradient Boosting builds a strong predictive model by adding weak models one after another. Each new model focuses on correcting the errors made by the previous models. The process happens step by step as follows.\n",
    "\n",
    "#### Step 1: Initialize the Model\n",
    "The process starts with a simple model that makes basic predictions.  \n",
    "For regression problems, this is usually the **mean of the target values**.  \n",
    "This initial model does not use any features and serves as a starting point.\n",
    "\n",
    "#### Step 2: Compute the Loss\n",
    "The predictions made by the initial model are compared with the actual target values using a **loss function**.  \n",
    "The loss function measures how wrong the predictions are.\n",
    "\n",
    "#### Step 3: Calculate the Residuals\n",
    "Residuals represent the **errors** made by the model.  \n",
    "They are calculated as the difference between the actual values and the predicted values.  \n",
    "These residuals tell us what the model failed to learn.\n",
    "\n",
    "#### Step 4: Train a Weak Learner on Residuals\n",
    "A new weak model (usually a shallow decision tree) is trained to predict the residuals instead of the original target values.  \n",
    "This model learns patterns in the errors made by the previous model.\n",
    "\n",
    "#### Step 5: Update the Model\n",
    "The predictions from the new weak learner are added to the existing model.  \n",
    "A **learning rate** is used to control how much influence the new model has.  \n",
    "This helps prevent overfitting.\n",
    "\n",
    "#### Step 6: Repeat the Process\n",
    "Steps 2 to 5 are repeated multiple times.  \n",
    "Each new model focuses on reducing the remaining errors left by the previous models.\n",
    "\n",
    "#### Step 7: Final Prediction\n",
    "The final model is a combination of all the weak learners.  \n",
    "Predictions are made by summing the contributions from each model in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb34c8-7237-472d-98a0-6a26ea02029d",
   "metadata": {},
   "source": [
    "### 3.Gradient Tape\n",
    "\n",
    "Gradient Tape is a mechanism provided by TensorFlow to support automatic differentiation. It is implemented as a context manager using `tf.GradientTape()`. When computations are performed inside this context, TensorFlow records all operations involving tensors. This recording allows TensorFlow to later compute gradients with respect to selected variables.\n",
    "\n",
    "The main idea behind Gradient Tape is simple: if TensorFlow knows how a value was computed, it can determine how that value changes when its inputs change. Gradient Tape keeps track of these dependencies by building a computational graph dynamically during execution.\n",
    "\n",
    "Only tensors that are being watched by the tape are considered for gradient computation. By default, trainable variables are automatically watched, but other tensors can also be explicitly monitored. Once the forward computation is complete, gradients can be calculated by asking the tape to compute derivatives of an output with respect to the desired input variables.\n",
    "\n",
    "Gradient Tape is especially useful because it allows developers to write normal Python code while still enabling backpropagation. This makes model development flexible and intuitive, particularly when experimenting with custom loss functions or training loops.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5fc360-c50f-40bd-b449-6cd8c9565fda",
   "metadata": {},
   "source": [
    "### 4.Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a function, most commonly a loss function in machine learning. The goal of training a model is to find parameter values that produce the smallest possible loss, and gradient descent provides a systematic way to achieve this.\n",
    "\n",
    "The key idea is based on the gradient, which represents the direction of the steepest increase of a function. To reduce the loss, parameters are updated in the opposite direction of the gradient. Each update moves the parameters slightly closer to the minimum of the loss function.\n",
    "\n",
    "This process is iterative. Starting from an initial set of parameters, gradients are computed, parameters are updated, and the loss is recalculated. Over many iterations, these small updates gradually improve the model’s performance.\n",
    "\n",
    "Gradient descent relies heavily on automatic differentiation. Without accurate gradients, the optimization process would not know how to adjust parameters. Together, automatic differentiation and gradient descent form the foundation of training neural networks.\n",
    "\n",
    "Different variations of gradient descent exist, such as batch gradient descent, stochastic gradient descent, and mini-batch gradient descent, but they all follow the same core principle of learning by moving in the direction that reduces error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445c65e-b2d9-4c67-97b0-50456757c4c2",
   "metadata": {},
   "source": [
    "## Task for the Reader  \n",
    "Using a deep learning framework of your choice (TensorFlow or PyTorch), perform the following tasks to understand tensors and their basic operations.\n",
    "\n",
    "a) Create a scalar tensor, a 1-D tensor, and a 2-D tensor. Print each tensor along with its shape and rank.\n",
    "\n",
    "b) Perform element-wise addition and element-wise multiplication on two tensors of the same shape.\n",
    "\n",
    "c) Multiply a tensor by a scalar value and observe how the values change.\n",
    "\n",
    "d) Access a specific element from a tensor using indexing and extract a sub-part of a tensor using slicing.\n",
    "\n",
    "e) Reshape a tensor into a different shape and explain how reshaping affects the tensor without changing the total number of elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762ec25-d74c-4eeb-b446-eaf3f0f7b87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
