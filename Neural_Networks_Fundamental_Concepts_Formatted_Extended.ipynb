{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750f9a9b",
   "metadata": {},
   "source": [
    "# Neural Networks – Fundamental Concepts and Linear Models\n",
    "\n",
    "This notebook explains the fundamental concepts of neural networks with clear theory, mathematical formulation, illustrative examples, Python syntax, and practice-oriented explanations. The structure and formatting follow a clean academic style suitable for coursework and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1702d583",
   "metadata": {},
   "source": [
    "## Biological Neuron\n",
    "\n",
    "A biological neuron is the basic unit of the nervous system. It is responsible for receiving, processing, and transmitting information.\n",
    "\n",
    "A biological neuron consists of:\n",
    "- **Dendrites** – receive signals from other neurons\n",
    "- **Cell body (Soma)** – processes and integrates signals\n",
    "- **Axon** – transmits signals to other neurons\n",
    "\n",
    "Information is transmitted as electrical impulses and chemical signals. Learning occurs by strengthening or weakening synaptic connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d72a28",
   "metadata": {},
   "source": [
    "## Artificial Neuron\n",
    "\n",
    "An artificial neuron is a **mathematical model** inspired by the biological neuron.\n",
    "\n",
    "It mimics biological behavior as follows:\n",
    "- Inputs → signals\n",
    "- Weights → strength of connections\n",
    "- Output → processed signal\n",
    "\n",
    "The artificial neuron performs a weighted sum of inputs followed by an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb79164",
   "metadata": {},
   "source": [
    "## Structure of an Artificial Neuron\n",
    "\n",
    "An artificial neuron computes its output in two main steps:\n",
    "\n",
    "1. **Linear combination**\n",
    "   z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b\n",
    "\n",
    "2. **Activation function**\n",
    "   a = f(z)\n",
    "\n",
    "Where:\n",
    "- x → input vector\n",
    "- w → weight vector\n",
    "- b → bias\n",
    "- f( ) → activation function\n",
    "\n",
    "This structure enables the neuron to learn from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e70bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simple artificial neuron\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "w = np.array([0.2, 0.5, -0.3])\n",
    "b = 0.1\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10912e7b",
   "metadata": {},
   "source": [
    "## Role of Weights and Bias\n",
    "\n",
    "### Weights\n",
    "- Control the **importance** of each input\n",
    "- Learned during training\n",
    "- Higher weight → stronger influence\n",
    "\n",
    "### Bias\n",
    "- Allows the neuron to **shift the activation**\n",
    "- Helps the model fit data better\n",
    "- Prevents forcing the output through the origin\n",
    "\n",
    "Together, weights and bias define the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of different weights\n",
    "x = np.array([2.0, 3.0])\n",
    "w1 = np.array([0.1, 0.1])\n",
    "w2 = np.array([0.9, 0.9])\n",
    "b = 0.5\n",
    "\n",
    "np.dot(w1, x) + b, np.dot(w2, x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e600c4",
   "metadata": {},
   "source": [
    "## Activation Functions and Non-Linearity\n",
    "\n",
    "Activation functions introduce **non-linearity** into neural networks.\n",
    "\n",
    "Why non-linearity is important:\n",
    "- Real-world data is non-linear\n",
    "- Linear models cannot capture complex patterns\n",
    "\n",
    "Common activation functions:\n",
    "- Step function\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- ReLU\n",
    "\n",
    "Without activation functions, deep networks behave like linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69800f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation example\n",
    "z = np.array([-3, -1, 0, 2, 4])\n",
    "relu = np.maximum(0, z)\n",
    "relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd978a2",
   "metadata": {},
   "source": [
    "## Single Neuron Model (Perceptron)\n",
    "\n",
    "The perceptron is the simplest neural network model used for binary classification.\n",
    "\n",
    "Decision rule:\n",
    "- Output = 1 if (w·x + b) ≥ 0\n",
    "- Output = 0 otherwise\n",
    "\n",
    "Limitations:\n",
    "- Works only for linearly separable data\n",
    "- Cannot solve XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd43bd",
   "metadata": {},
   "source": [
    "## Linear Transformation in Neural Networks (Wx + b)\n",
    "\n",
    "Linear transformation is the core computation in neural networks.\n",
    "\n",
    "Matrix form:\n",
    "z = Wx + b\n",
    "\n",
    "This allows efficient computation of multiple neurons simultaneously and supports GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-based linear transformation\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "W = np.array([[0.2, 0.4], [0.6, 0.8]])\n",
    "b = np.array([0.1, 0.2])\n",
    "np.dot(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfca0a",
   "metadata": {},
   "source": [
    "## From Single Neuron to Fully Connected (Linear) Layer\n",
    "\n",
    "A fully connected layer consists of multiple neurons.\n",
    "\n",
    "Characteristics:\n",
    "- Each neuron connects to all inputs\n",
    "- Each neuron has its own weights and bias\n",
    "- Enables learning of multiple features\n",
    "\n",
    "Fully connected layers are widely used in multilayer perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd842a",
   "metadata": {},
   "source": [
    "## Layers in a Neural Network\n",
    "\n",
    "### Input Layer\n",
    "- Receives raw input features\n",
    "- No trainable parameters\n",
    "\n",
    "### Hidden Layer\n",
    "- Performs feature extraction\n",
    "- Applies linear and non-linear transformations\n",
    "\n",
    "### Output Layer\n",
    "- Produces final prediction\n",
    "- Depends on task (classification or regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778856e",
   "metadata": {},
   "source": [
    "## Forward Pass: Flow of Information Through the Network\n",
    "\n",
    "The forward pass describes how data flows through the neural network.\n",
    "\n",
    "Steps involved:\n",
    "1. Input data enters the network\n",
    "2. Linear transformation is applied\n",
    "3. Activation function is applied\n",
    "4. Output is produced\n",
    "\n",
    "The forward pass is used during both training and inference."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
