{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43e770c-d0be-4122-b90c-d8e6820092f4",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a class of deep learning models that are especially effective for image-related tasks such as image classification, object detection, and facial recognition.\n",
    "\n",
    "They are inspired by how humans learn to recognise objects and how the human visual system processes visual information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4db1e0-be31-4f57-b755-885ce73e1613",
   "metadata": {},
   "source": [
    "\n",
    "## How Machines Learn to See Images\n",
    "\n",
    "Similar to how a child learns to recognise objects, a machine learning algorithm must be shown a very large number of images before it can generalise. Only after seeing enough examples can it make correct predictions for images it has never seen before.\n",
    "\n",
    "However, computers see the world very differently from humans.\n",
    "\n",
    "A computer does not see objects, shapes, or meaning. Its world consists entirely of numbers. Every image can be represented as a two-dimensional array of numbers, where each number corresponds to a pixel value.\n",
    "\n",
    "Even though computers perceive images differently, this does not mean they cannot learn patterns. It simply means that we must represent images in a way that allows algorithms to discover meaningful structures from numerical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421661b-04bc-4283-8121-36e47b8f0188",
   "metadata": {},
   "source": [
    "## Why Regular Neural Networks Are Not Enough\n",
    "\n",
    "A regular (fully connected) neural network treats every input value as independent. When working with images, this usually means flattening the image into a one-dimensional vector before feeding it into the network.\n",
    "\n",
    "Flattening an image causes a major problem: all spatial information is lost. The network no longer knows which pixels were originally close to each other or how they formed shapes such as edges or corners.\n",
    "\n",
    "As a result, a traditional neural network cannot effectively capture patterns like textures, edges, or object structures. This makes it inefficient and poorly suited for image-related tasks.\n",
    "\n",
    "Convolutional Neural Networks solve this problem by preserving spatial relationships and learning features directly from local regions of the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac40cc9-5baa-4482-9615-a77977400d59",
   "metadata": {},
   "source": [
    "## Biological Inspiration of Convolutional Neural Networks\n",
    "\n",
    "Convolutional Neural Networks are strongly inspired by the way the human visual system processes visual information. Studies conducted in the 1950s and 1960s by neuroscientists Hubel and Wiesel revealed important insights into how visual perception works in mammals.\n",
    "\n",
    "Their research showed that neurons in the visual cortex do not respond to the entire visual field at once. Instead, each neuron is activated only by stimuli within a small, specific region of the visual field. This concept is known as a receptive field.\n",
    "\n",
    "They identified two main types of visual neurons. The first type, called simple cells, respond to basic visual patterns such as straight lines or edges at specific orientations and positions. The second type, known as complex cells, respond to the same patterns even when their position changes slightly within the visual field.\n",
    "\n",
    "This hierarchical and local processing of visual information inspired the design of Convolutional Neural Networks. Similar to the human brain, CNNs analyse images by detecting simple features first and gradually combining them to form more complex representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd91b49-a72c-4fd4-8ffa-fff5e7c709f6",
   "metadata": {},
   "source": [
    "## Architecture of Convolutional Neural Networks\n",
    "\n",
    "The architecture of a Convolutional Neural Network (CNN) is fundamentally different from that of a traditional fully connected neural network. CNNs are specifically designed to process data that has a grid-like structure, such as images.\n",
    "\n",
    "Unlike regular neural networks, which treat all input features equally, CNNs exploit the spatial structure of images. This allows them to learn visual patterns more efficiently while significantly reducing the number of parameters required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea4fa3-ab6b-4fab-a4b9-e0c78afa1303",
   "metadata": {},
   "source": [
    "![cnn architecture](https://media.licdn.com/dms/image/v2/D5612AQGOui8XZUZJSA/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1680532048475?e=2147483647&v=beta&t=5gZVHYNL2Vc2mK3iKrpK-FcpURIFdyaP4Vi38eeeZyM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b4e67-8c3b-4bae-9b3e-58fde3d89389",
   "metadata": {},
   "source": [
    "\n",
    "## Components of a Convolutional Neural Network\n",
    "\n",
    "A Convolutional Neural Network is not a single, uniform block of computation. Instead, it is organised into two major components, each responsible for a distinct role in the overall learning process.\n",
    "\n",
    "The first component is responsible for understanding the visual content of an image. It analyses the raw pixel values and learns to identify meaningful visual patterns such as edges, textures, shapes, and object parts. This component is commonly referred to as the **feature extraction component**.\n",
    "\n",
    "The second component is responsible for interpreting the features learned by the first component. Using these extracted features, it determines what the image represents by assigning probabilities to different output classes. This component is known as the **classification component**.\n",
    "\n",
    "By dividing the network into these two components, CNNs are able to efficiently process images, first by learning *what* is present in the image and then by deciding *what the image means*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb76817-38e3-42e0-95d4-dea598fc1ff6",
   "metadata": {},
   "source": [
    "## Feature Extraction Component\n",
    "\n",
    "The feature extraction component is the first and most important part of a Convolutional Neural Network. Its primary role is to transform raw image data into meaningful representations that can be understood and used by the network.\n",
    "\n",
    "When an image is first given as input to a CNN, it exists only as a grid of numerical pixel values. At this stage, the network has no knowledge of shapes, objects, or patterns. Individual pixel values by themselves carry very little useful information. The task of the feature extraction component is to discover structure and patterns within this raw data.\n",
    "\n",
    "This component works by gradually analysing small regions of the image and learning visual features at multiple levels of complexity. In the earliest stages, the network learns very simple features such as edges, lines, and corners. These features are fundamental building blocks and appear in almost all images, regardless of the object being depicted.\n",
    "\n",
    "As the image data moves deeper through the feature extraction component, these simple features are combined to form more complex patterns such as textures, curves, and basic shapes. In even deeper layers, the network learns high-level representations such as object parts or distinctive visual characteristics that are strongly associated with specific classes.\n",
    "\n",
    "The feature extraction component does not rely on manually defined rules or handcrafted features. Instead, it automatically learns which features are important directly from the training data. This ability to learn features hierarchically and automatically is one of the key strengths of Convolutional Neural Networks and is a major reason for their success in image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a278b60-9ede-4ab3-aa50-44aa21c22391",
   "metadata": {},
   "source": [
    "## Convolution Operation in Convolutional Neural Networks\n",
    "\n",
    "Convolution is one of the most important building blocks of a Convolutional Neural Network. It is the primary mechanism through which the network learns visual features from an image.\n",
    "\n",
    "In mathematics, convolution refers to an operation that combines two functions to produce a third function. Conceptually, it merges two sources of information. In the context of CNNs, these two sources are:\n",
    "1. The input image\n",
    "2. A small matrix called a filter or kernel\n",
    "\n",
    "The result of applying a filter to an image through convolution is known as a **feature map**.\n",
    "In the animation below, you can see the convolution operation. You can see the filter (the green square) is sliding over our input (the blue square) and the sum of the convolution goes into the feature map (the red square).\n",
    "\n",
    "The area of our filter is also called the receptive field, named after the neuron cells! The size of this filter is 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df312f83-2c69-4e35-a5f8-86b6080a56f7",
   "metadata": {},
   "source": [
    "![convolution](https://cdn-media-1.freecodecamp.org/images/Htskzls1pGp98-X2mHmVy9tCj0cYXkiCrQ4t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f292b3-783d-4d10-a7fc-d6c6ecaae258",
   "metadata": {},
   "source": [
    "## Filter (Kernel) and Input Image\n",
    "\n",
    "A filter, also called a kernel, is a small matrix of learnable values. Common filter sizes are 3×3 or 5×5. The filter is much smaller than the input image and is designed to focus on local regions rather than the entire image at once.\n",
    "\n",
    "The input image is represented as a matrix of pixel values. For a grayscale image, this matrix has two dimensions (height and width). For a colour image, it has three dimensions (height, width, and depth).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a823816-6b6f-4c26-bbeb-2cdf552be30f",
   "metadata": {},
   "source": [
    "The convolution operation is performed by sliding the filter over the input image.\n",
    "\n",
    "At each position:\n",
    "- The filter is placed over a small region of the image\n",
    "- Element-wise multiplication is performed between the filter values and the corresponding pixel values\n",
    "- All resulting values are summed to produce a single number\n",
    "\n",
    "This single number is placed into the corresponding position in the feature map.\n",
    "\n",
    "This process is repeated as the filter moves across the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa25334-1bc2-4e40-b10d-27a15bd53242",
   "metadata": {},
   "source": [
    "## Receptive Field\n",
    "\n",
    "The region of the image that the filter covers at any given time is called the **receptive field**.\n",
    "\n",
    "For example:\n",
    "- A 3×3 filter has a receptive field of size 3×3\n",
    "- This means the neuron associated with that output value only looks at a 3×3 region of the input image\n",
    "\n",
    "This concept is inspired by biological vision, where neurons respond only to stimuli within a small region of the visual field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee48c40-e2ae-48e7-b77b-f44097b8472f",
   "metadata": {},
   "source": [
    "## From 2D to 3D Convolution\n",
    "\n",
    "For simplicity, convolution is often illustrated using two-dimensional images. However, in practice, convolution is performed in **three dimensions**.\n",
    "\n",
    "A colour image has three channels: Red, Green, and Blue. Therefore:\n",
    "- The filter also has a depth of 3\n",
    "- The filter spans the entire depth of the input image\n",
    "- The convolution operation produces a two-dimensional feature map\n",
    "\n",
    "Each filter always covers all input channels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873283eb-e675-4eae-8332-8d24f9436d6a",
   "metadata": {},
   "source": [
    "## Multiple Filters and Feature Maps\n",
    "\n",
    "A single filter can detect only one type of feature. To learn multiple features, a convolution layer uses **many filters**.\n",
    "\n",
    "Each filter:\n",
    "- Learns a different visual pattern\n",
    "- Produces its own feature map\n",
    "\n",
    "All feature maps are stacked together to form the output of the convolution layer. The number of feature maps equals the number of filters used.\n",
    "![3d](https://cdn-media-1.freecodecamp.org/images/Gjxh-aApWTzIRI1UNmGnNLrk8OKsQaf2tlDu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82473a21-d252-47c4-b3c5-421eee438f73",
   "metadata": {},
   "source": [
    "## Activation Function After Convolution\n",
    "\n",
    "After convolution, the output values are passed through an activation function to introduce non-linearity.\n",
    "\n",
    "In CNNs, the most commonly used activation function is the **ReLU (Rectified Linear Unit)**.\n",
    "\n",
    "ReLU replaces all negative values with zero while keeping positive values unchanged. This allows the network to learn complex patterns and improves training efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a29327-d512-4154-85d0-d7f7c2af48f3",
   "metadata": {},
   "source": [
    "## Stride: How the Filter Moves\n",
    "\n",
    "Stride defines how many pixels the filter moves at each step.\n",
    "\n",
    "- A stride of 1 means the filter moves one pixel at a time\n",
    "- Larger stride values cause the filter to move in larger steps\n",
    "\n",
    "Increasing the stride reduces the size of the feature map and lowers computational cost, but may also result in loss of fine-grained information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ca715-bb33-4caf-a303-5d89bc8fcb9a",
   "metadata": {},
   "source": [
    "## Padding: Preserving Spatial Size\n",
    "\n",
    "After convolution, the feature map is usually smaller than the input image. To control this size reduction, **padding** is used.\n",
    "\n",
    "Padding adds a border of zero-valued pixels around the input image. This allows:\n",
    "- Preservation of spatial dimensions\n",
    "- Better handling of edge information\n",
    "- Proper alignment of filters with the image\n",
    "\n",
    "Padding is especially useful when multiple convolution layers are stacked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0f41d-a818-4394-a8ef-e4b070e47b39",
   "metadata": {},
   "source": [
    "## Pooling Layer After Convolution\n",
    "\n",
    "After one or more convolution layers, a pooling layer is commonly added.\n",
    "\n",
    "Pooling reduces the spatial dimensions of feature maps, which:\n",
    "- Decreases the number of parameters\n",
    "- Reduces computation\n",
    "- Helps control overfitting\n",
    "\n",
    "The most common pooling operation is **max pooling**, where the maximum value within a window is selected.\n",
    "\n",
    "![](https://cdn-media-1.freecodecamp.org/images/96HH3r99NwOK818EB9ZdEbVY3zOBOYJE-I8Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec90b89-4979-4f28-82a6-3452cbb8e0bd",
   "metadata": {},
   "source": [
    "When designing a convolution layer, four key hyperparameters must be chosen carefully:\n",
    "- Kernel size\n",
    "- Number of filters\n",
    "- Stride\n",
    "- Padding\n",
    "\n",
    "These parameters directly influence the feature map size, learning capacity, and performance of the CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f4945-dfcd-4a53-aaa1-8c9d9dbc9f2b",
   "metadata": {},
   "source": [
    "## Classification Component of a Convolutional Neural Network\n",
    "\n",
    "The classification component is the second major part of a Convolutional Neural Network. While the feature extraction component focuses on learning *what visual patterns exist* in the image, the classification component focuses on *what those patterns mean*.\n",
    "\n",
    "By the time the data reaches the classification component, the original image has already been transformed into a set of high-level features. These features encode important information about the image, such as the presence of edges, shapes, textures, and object parts. The role of the classification component is to interpret these learned features and make a final decision about the class of the input image.\n",
    "\n",
    "In essence, the classification component answers the question:  \n",
    "**“Given the features that have been extracted, what is the image?”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fd032-ef77-417d-bb5d-c3f7090ace30",
   "metadata": {},
   "source": [
    "## Fully Connected Layers in the Classification Component\n",
    "\n",
    "The core of the classification component consists of one or more fully connected (dense) layers. These layers operate in a similar manner to traditional neural networks.\n",
    "\n",
    "In a fully connected layer, each neuron is connected to every neuron in the previous layer. This complete connectivity allows the network to combine all extracted features and learn complex relationships between them.\n",
    "\n",
    "Unlike convolution layers, which focus on local patterns, fully connected layers consider the entire set of learned features simultaneously. This makes them well suited for high-level reasoning and decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad1860-dde7-4251-b928-c44060911dea",
   "metadata": {},
   "source": [
    "## Purpose of Fully Connected Layers\n",
    "\n",
    "The features extracted by convolution and pooling layers are spatial and distributed across feature maps. While these features are useful, they do not directly correspond to class predictions.\n",
    "\n",
    "Fully connected layers serve as a classifier that:\n",
    "- Combines all extracted features\n",
    "- Learns decision boundaries between classes\n",
    "- Produces scores corresponding to each class\n",
    "\n",
    "Through training, the fully connected layers learn how different feature combinations relate to different output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f57b0b-1d0d-48cd-a4f7-ecff902c6e15",
   "metadata": {},
   "source": [
    "## Output Layer in the Classification Component\n",
    "\n",
    "The final layer of the classification component is known as the output layer. This layer produces a numerical score for each possible class.\n",
    "\n",
    "Each neuron in the output layer corresponds to one class. The values produced by this layer indicate how strongly the network believes the input image belongs to each class.\n",
    "\n",
    "These raw scores are not yet probabilities and must be further processed using an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbc220-fa80-499b-99cf-fb43a760c877",
   "metadata": {},
   "source": [
    "## Softmax Activation Function\n",
    "\n",
    "The softmax activation function is commonly used in the output layer of a CNN for multi-class classification tasks.\n",
    "\n",
    "Softmax converts raw output scores into probabilities. Each probability lies between 0 and 1, and the sum of all probabilities equals 1.\n",
    "\n",
    "This makes the output interpretable, as it directly represents the model’s confidence in each class. The class with the highest probability is selected as the final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f40f95-1183-435d-a1c9-26447465f04d",
   "metadata": {},
   "source": [
    "## Final Interpretation of the CNN Output\n",
    "\n",
    "After the softmax activation function is applied, the CNN produces a probability distribution over all possible classes.\n",
    "\n",
    "The predicted class is the one with the highest probability. This probabilistic output not only provides a prediction but also indicates how confident the network is in its decision.\n",
    "\n",
    "This marks the completion of the classification component and the final stage of the Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd27a90-2c01-4de4-b99f-f720eb701f23",
   "metadata": {},
   "source": [
    "## Training a Convolutional Neural Network\n",
    "\n",
    "Training a Convolutional Neural Network is the process through which the network learns the values of its parameters so that it can make accurate predictions.\n",
    "\n",
    "During training, the CNN is shown a large number of input images along with their correct labels. Based on these examples, the network gradually adjusts its internal parameters to minimise prediction errors.\n",
    "\n",
    "Although CNNs have a specialised architecture, the fundamental training principle is the same as that of traditional neural networks: learning through error correction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99ebf8-7cbe-49ca-acd9-dc3c181bffe3",
   "metadata": {},
   "source": [
    "\n",
    "## What a CNN Learns During Training\n",
    "\n",
    "A CNN contains two types of layers:\n",
    "- Layers with learnable parameters (convolution and fully connected layers)\n",
    "- Layers without learnable parameters (activation, pooling, flattening)\n",
    "\n",
    "During training:\n",
    "- Convolution layers learn filter values\n",
    "- Fully connected layers learn weights and biases\n",
    "\n",
    "Pooling and activation layers do not learn parameters; they only transform data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515a7b9-5e18-4503-afc4-3d7fd8091ad4",
   "metadata": {},
   "source": [
    "## Forward Propagation in a CNN\n",
    "\n",
    "Forward propagation is the process of passing an input image through the entire CNN from start to end.\n",
    "\n",
    "The input image first passes through the feature extraction component, where convolution, activation, and pooling operations extract meaningful features. These features are then flattened and passed through the classification component, producing output scores for each class.\n",
    "\n",
    "At the end of forward propagation, the network produces a prediction based on its current parameter values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefb6e7-4bd3-454f-a0fe-d5c7a9ecc6aa",
   "metadata": {},
   "source": [
    "## Loss Function in CNN Training\n",
    "\n",
    "The loss function measures how far the network’s prediction is from the true label.\n",
    "\n",
    "It produces a single numerical value that represents the error made by the network for a given input. A smaller loss value indicates better performance.\n",
    "\n",
    "In image classification tasks, common loss functions include:\n",
    "- Categorical Cross-Entropy\n",
    "- Sparse Categorical Cross-Entropy\n",
    "\n",
    "The objective of training is to minimise the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd0d97-e4e1-492e-888d-ba3e6931a36d",
   "metadata": {},
   "source": [
    "## Why Cross-Entropy Loss Is Used\n",
    "\n",
    "Cross-entropy loss is well suited for classification problems because it strongly penalises confident but incorrect predictions.\n",
    "\n",
    "When the network assigns high probability to the wrong class, the loss becomes large. When it assigns high probability to the correct class, the loss becomes small.\n",
    "\n",
    "This behaviour encourages the network to produce accurate and confident predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ef489-6823-4aa5-8685-c068fde19cb3",
   "metadata": {},
   "source": [
    "## Backpropagation in Convolutional Neural Networks\n",
    "\n",
    "Backpropagation is the process through which the network updates its parameters to reduce the loss.\n",
    "\n",
    "After forward propagation and loss calculation, the error is propagated backward through the network. During this process, gradients of the loss function with respect to each parameter are computed.\n",
    "\n",
    "In CNNs, backpropagation updates:\n",
    "- Filter values in convolution layers\n",
    "- Weights and biases in fully connected layers\n",
    "\n",
    "Although the mathematical details are more complex due to convolution operations, the underlying principle remains the same as in traditional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949270b-da5b-4efd-b076-48578bed0467",
   "metadata": {},
   "source": [
    "## Gradient Descent and Optimization\n",
    "\n",
    "Once gradients are computed through backpropagation, an optimization algorithm is used to update the parameters.\n",
    "\n",
    "Gradient descent updates parameters in the direction that reduces the loss. The size of each update is controlled by the learning rate.\n",
    "\n",
    "Choosing an appropriate learning rate is crucial:\n",
    "- Too large → unstable training\n",
    "- Too small → slow learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f15313-e5cb-45bd-ae51-bf7aa9a034a1",
   "metadata": {},
   "source": [
    "## Optimizers in CNN Training\n",
    "\n",
    "Optimizers determine how parameter updates are performed.\n",
    "\n",
    "One of the most commonly used optimizers is Adam (Adaptive Moment Estimation). Adam combines the advantages of momentum and adaptive learning rates.\n",
    "\n",
    "Adam automatically adjusts learning rates for each parameter, leading to faster and more stable convergence in CNN training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4347cf4-3457-4265-9dde-ca90efd3ea58",
   "metadata": {},
   "source": [
    "## Epochs and Batches\n",
    "\n",
    "Training data is not processed all at once.\n",
    "\n",
    "- A batch is a small subset of the training data\n",
    "- An epoch is one complete pass through the entire training dataset\n",
    "\n",
    "Training over multiple epochs allows the network to refine its parameters gradually and improve accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4d779-5630-4f98-81f3-55ca734305d9",
   "metadata": {},
   "source": [
    "## Training and Validation Data\n",
    "\n",
    "During training, data is typically divided into:\n",
    "- Training data, used to update parameters\n",
    "- Validation data, used to evaluate performance during training\n",
    "\n",
    "Validation helps monitor overfitting and ensures that the model generalises well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce1c31-dd8a-47cb-a11e-179af2d3e6b3",
   "metadata": {},
   "source": [
    "Before training begins, the CNN must be compiled.\n",
    "\n",
    "Compilation involves specifying:\n",
    "- The loss function\n",
    "- The optimizer\n",
    "- The evaluation metric\n",
    "\n",
    "These choices define how the model learns and how its performance is measured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5673c8f-b869-45d9-9573-223358beec0c",
   "metadata": {},
   "source": [
    "Only through training, the CNN learns to extract meaningful features and make accurate predictions on image data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc8ca45-2ea1-4c18-9d91-c211d9ef38ce",
   "metadata": {},
   "source": [
    "\n",
    "# Example: Image Classification Using Convolutional Neural Network (CNN)\n",
    "\n",
    "In this section, let us look at the implementation of a Convolutional Neural Network (CNN) for image classification using the MNIST dataset. The objective of this example is to understand how CNNs are applied in practice to classify images by learning hierarchical visual features and making accurate predictions.\n",
    "\n",
    "The complete workflow includes dataset overview, data preprocessing, model construction, training, evaluation, and prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e57110-4705-41e7-bce3-cd3b2b1d30db",
   "metadata": {},
   "source": [
    "## Characteristics of the MNIST Dataset\n",
    "The MNIST dataset (Modified National Institute of Standards and Technology dataset) is one of the most widely used benchmark datasets in the field of machine learning and deep learning. It is primarily used for training and evaluating image classification models, especially Convolutional Neural Networks.\n",
    "\n",
    "The dataset consists of images of handwritten digits ranging from 0 to 9. Each image represents a single digit written by different individuals, making the dataset suitable for studying pattern recognition and generalization.\n",
    "\n",
    "- The dataset contains 70,000 images in total  \n",
    "- 60,000 images are used for training  \n",
    "- 10,000 images are used for testing  \n",
    "- Each image is of size 28 × 28 pixels\n",
    "- Images are grayscale, meaning they have a single color channel  \n",
    "- Pixel values range from 0 to 255, where 0 represents black and 255 represents white  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d3af1-01cb-432e-a13f-a88073696083",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "NumPy is used for numerical operations and array manipulation.  \n",
    "Matplotlib is used for visualizing image data.  \n",
    "TensorFlow Keras provides high-level APIs to build, train, and evaluate Convolutional Neural Networks, including layers such as convolution, pooling, and fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd0f214-a709-493f-887d-45ec2160952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952484ad-b1a4-4f0e-a09e-5a22e0b61c76",
   "metadata": {},
   "source": [
    "### Loading the MNIST Dataset\n",
    "\n",
    "The MNIST dataset is loaded directly using Keras.  \n",
    "It is automatically split into training data (60,000 images) and testing data (10,000 images).  \n",
    "Each image is a grayscale handwritten digit with a corresponding label from 0 to 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d67ad72-0113-4510-a9f7-9ff0c89c93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb91dd14-d36c-461a-bb71-2ae881480733",
   "metadata": {},
   "source": [
    "### Understanding the Dataset Shape\n",
    "\n",
    "The training images have the shape (60000, 28, 28), meaning there are 60,000 images of size 28 × 28 pixels.  \n",
    "The labels have the shape (60000,), indicating one label per image.  \n",
    "Since CNNs expect channel information, the data must be reshaped before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4a2c88-84f0-4d07-a997-d800e437fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a7b93-79c0-4579-a056-3b20c18a7f58",
   "metadata": {},
   "source": [
    "### Reshaping the Data\n",
    "\n",
    "Convolutional Neural Networks expect input in the format:  \n",
    "(height, width, channels).\n",
    "\n",
    "Since MNIST images are grayscale, they have only one channel.  \n",
    "Reshaping adds this channel dimension and makes the data compatible with CNN layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73011228-a6c9-4c59-8ed6-208a79598c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e46adc-4310-41af-8fe2-fa63147a3130",
   "metadata": {},
   "source": [
    "### Normalizing the Data\n",
    "\n",
    "Pixel values in MNIST range from 0 to 255.  \n",
    "Dividing by 255 scales the values to the range 0 to 1.  \n",
    "Normalization improves training stability and speeds up convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f5f7ad-5bf3-449a-a856-c98b336d3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a1e21-19a4-4656-8d91-ad3ad5f8c056",
   "metadata": {},
   "source": [
    "### Creating the CNN Model\n",
    "\n",
    "The Sequential model is used to build the CNN layer by layer in a linear stack.  \n",
    "This approach is simple and well suited for standard CNN architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01da2206-c4b3-4796-b815-999d61a046ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3f049d-e618-4027-8eb4-37470cece73d",
   "metadata": {},
   "source": [
    "### First Convolution Block\n",
    "\n",
    "The convolution layer applies 32 filters of size 3 × 3 to extract basic visual features such as edges.  \n",
    "The ReLU activation introduces non-linearity, allowing the network to learn complex patterns.  \n",
    "Max pooling reduces the spatial dimensions and helps control overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074ae50d-237e-4a44-9dc7-9da783de45f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nausheen\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(32, (3,3), input_shape=(28,28,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9884ee-b0a6-46f9-840b-8f27731cd583",
   "metadata": {},
   "source": [
    "### Second Convolution Block\n",
    "\n",
    "The second convolution layer learns more complex features by building on the features extracted earlier.  \n",
    "Increasing the number of filters allows the network to capture richer visual information.  \n",
    "Pooling again reduces the feature map size while retaining important details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc546f6-b397-482d-8d85-939cd5f267bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c2b2f-6af8-406d-ba8e-53be85655a8d",
   "metadata": {},
   "source": [
    "### Flattening the Feature Maps\n",
    "\n",
    "Flattening converts the three-dimensional feature maps into a one-dimensional vector.  \n",
    "This step is required before passing the data to fully connected layers for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272aadc3-49cd-4f39-84a3-6757becebabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d2ae4-add5-4c55-b1fb-218712a3fae0",
   "metadata": {},
   "source": [
    "### Fully Connected Layer\n",
    "\n",
    "The dense layer combines all extracted features and learns relationships between them.  \n",
    "This layer performs high-level reasoning and prepares the data for final classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6e8f4f-aa94-4222-af7d-57991e9ffc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6e52e-ee31-4572-a2a4-6d8944c7d703",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "The output layer has 10 neurons, one for each digit class (0–9).  \n",
    "The softmax activation converts outputs into probabilities, enabling multi-class classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db79b7e9-a4d8-425c-aade-81387df6b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b22141-2252-4fcf-aa7a-af3fab660a35",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "\n",
    "The loss function measures classification error.  \n",
    "The Adam optimizer efficiently updates model parameters.  \n",
    "Accuracy is used to evaluate the model’s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ce14b7-a563-4d27-a6b1-1531b0121665",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed5ac2-01e0-4560-b795-1b57e2ea45c3",
   "metadata": {},
   "source": [
    "### Training the CNN\n",
    "\n",
    "The model is trained using the training dataset for multiple epochs.  \n",
    "Training adjusts the filters and weights to minimize loss and improve accuracy.  \n",
    "Validation data is used to monitor generalization performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e0b954-c1ba-4746-8637-03c358ce70de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 35ms/step - accuracy: 0.9606 - loss: 0.1277 - val_accuracy: 0.9866 - val_loss: 0.0419\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 34ms/step - accuracy: 0.9866 - loss: 0.0431 - val_accuracy: 0.9886 - val_loss: 0.0320\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 34ms/step - accuracy: 0.9908 - loss: 0.0284 - val_accuracy: 0.9901 - val_loss: 0.0271\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 32ms/step - accuracy: 0.9937 - loss: 0.0206 - val_accuracy: 0.9896 - val_loss: 0.0331\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 34ms/step - accuracy: 0.9945 - loss: 0.0167 - val_accuracy: 0.9901 - val_loss: 0.0305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24de537c830>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900ea13-fe86-497f-b7b8-a79e70f38211",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "The trained CNN is evaluated using unseen test data.  \n",
    "The test accuracy indicates how well the model generalizes to new images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27cf9b47-a073-4b46-aa2f-cd09f40b6f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9901 - loss: 0.0305\n",
      "Test Accuracy: 0.9901000261306763\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be25a9-cde8-4dc9-8563-3f8f2b9c298e",
   "metadata": {},
   "source": [
    "In this example, a Convolutional Neural Network was built to classify handwritten digits from the MNIST dataset.  \n",
    "The model used convolution and pooling layers for feature extraction and fully connected layers for classification.  \n",
    "This demonstrates the complete workflow of image classification using CNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b135b11-344a-41f2-a74e-e26f8358aca2",
   "metadata": {},
   "source": [
    "\n",
    "## Task for the Reader\n",
    "1. Modify the CNN architecture by changing the number of convolution filters and observe how it affects the training and test accuracy.\n",
    "\n",
    "2. Increase or decrease the number of training epochs and analyze the impact on model performance and overfitting.\n",
    "\n",
    "3. Replace the max pooling layer with average pooling and compare the results obtained during testing.\n",
    "\n",
    "4. Test the trained model on multiple individual images from the test dataset and identify cases where the model makes incorrect predictions.\n",
    "\n",
    "5. Experiment with different optimizers such as SGD or RMSprop and compare their convergence behavior with Adam.\n",
    "\n",
    "6. Try visualizing the training and validation accuracy curves to better understand how the model learns over time.\n",
    "\n",
    "Completing these tasks will help deepen your practical understanding of how CNN design choices influence image classification performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
