{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0717b078",
   "metadata": {},
   "source": [
    "\n",
    "# 0.8 Model Training, Evaluation, and Regularization\n",
    "\n",
    "This notebook introduces the complete workflow of training a simple neural network model, evaluating its performance, and applying regularization techniques to improve generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2de01",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction to a Simple Classification Dataset\n",
    "\n",
    "A **classification dataset** consists of input features and corresponding class labels.\n",
    "\n",
    "Typical examples:\n",
    "- Email spam detection (spam / not spam)\n",
    "- Handwritten digit recognition (0–9)\n",
    "- Disease prediction (yes / no)\n",
    "\n",
    "**Mathematically:**  \n",
    "- Input matrix: $X \\in \\mathbb{R}^{m \\times n}$  \n",
    "- Label vector: $y$\n",
    "\n",
    "\n",
    "\n",
    "**Goal:** Learn a mapping from inputs to class labels that generalizes well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f901a2",
   "metadata": {},
   "source": [
    "\n",
    "## Data Preparation and Preprocessing\n",
    "\n",
    "Data preparation and preprocessing are critical steps in the machine learning pipeline. Real-world data is often incomplete, noisy, or inconsistent, and cannot be directly used for training a neural network.\n",
    "\n",
    "Before training, raw data must be cleaned and transformed.\n",
    "\n",
    "Common steps:\n",
    "- Handling missing values  \n",
    "- Feature scaling (Normalization / Standardization)  \n",
    "- Encoding categorical variables  \n",
    "- Train–Validation–Test split  \n",
    "\n",
    "Good preprocessing leads to **faster convergence** and **stable learning**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: simple normalization\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([10, 20, 30, 40])\n",
    "X_norm = (X - X.min()) / (X.max() - X.min())\n",
    "X_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68356a24",
   "metadata": {},
   "source": [
    "\n",
    "## Designing a Simple MLP Model\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) is a feedforward neural network composed of multiple layers of neurons. It includes an input layer, one or more hidden layers, and an output layer. Each neuron in a layer is connected to neurons in the subsequent layer through weighted connections.\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** consists of:\n",
    "- Input layer\n",
    "- One or more hidden layers\n",
    "- Output layer\n",
    "\n",
    "For classification:\n",
    "- Sigmoid → Binary classification\n",
    "- Softmax → Multi-class classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549dacd5",
   "metadata": {},
   "source": [
    "\n",
    "### MLP structure\n",
    "![mlp](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/640px-Artificial_neural_network.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c525561",
   "metadata": {},
   "source": [
    "\n",
    "## Forward Pass and Prediction\n",
    "\n",
    "The forward pass is the process by which input data is propagated through the neural network to generate an output. At each neuron, a weighted sum of inputs is computed and a bias term is added. This result is then passed through an activation function to produce the neuron’s output.\n",
    "\n",
    "Each neuron computes:\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "$$\n",
    "\n",
    "\n",
    "The result is passed through an **activation function** to produce output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple forward pass example\n",
    "x = np.array([1.0, 2.0])\n",
    "w = np.array([0.5, -1.0])\n",
    "b = 0.1\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab294bd",
   "metadata": {},
   "source": [
    "\n",
    "## Model Evaluation Using Accuracy\n",
    "\n",
    "Once a model has been trained, its performance must be evaluated using appropriate metrics. Accuracy is one of the simplest and most commonly used evaluation metrics for classification problems.\n",
    "\n",
    "Accuracy measures how many predictions are correct:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "\n",
    "Accuracy alone may be misleading for imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy example\n",
    "y_true = np.array([1, 0, 1, 1])\n",
    "y_pred = np.array([1, 0, 0, 1])\n",
    "\n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bfab99",
   "metadata": {},
   "source": [
    "\n",
    "## Underfitting vs Overfitting\n",
    "\n",
    "Underfitting and overfitting are two fundamental challenges in model training.\n",
    "\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. Such a model performs poorly on both training and unseen data, indicating insufficient learning capacity.\n",
    "\n",
    "Overfitting occurs when a model is excessively complex and learns not only the true patterns but also the noise present in the training data. An overfitted model shows excellent performance on training data but fails to generalize to new data.\n",
    "\n",
    "\n",
    "- **Underfitting:** Model too simple → poor performance everywhere  \n",
    "- **Overfitting:** Model too complex → memorizes training data  \n",
    "\n",
    "The primary objective of model training is to achieve a balance between these two extremes, resulting in good generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76499169",
   "metadata": {},
   "source": [
    "\n",
    "### Visual comparison\n",
    "![overfitting](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Overfitting.svg/640px-Overfitting.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f7306",
   "metadata": {},
   "source": [
    "\n",
    "## Regularization Techniques\n",
    "\n",
    "Regularization refers to a collection of methods designed to reduce overfitting by controlling model complexity. These techniques impose constraints or penalties that discourage the model from learning overly complex representations.\n",
    "\n",
    "Regularization prevents overfitting by limiting model complexity.\n",
    "\n",
    "Techniques include:\n",
    "- L1 / L2 Regularization\n",
    "- Dropout\n",
    "- Data Augmentation\n",
    "- Early Stopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd1a31d",
   "metadata": {},
   "source": [
    "\n",
    "## L1 and L2 Regularization\n",
    "\n",
    "L1 and L2 regularization are weight-based regularization techniques that modify the loss function by adding a penalty term.\n",
    "\n",
    "L1 regularization adds the sum of the absolute values of the model weights to the loss function. This encourages sparsity in the model by driving some weights to exactly zero, effectively performing feature selection.\n",
    "\n",
    "L2 regularization adds the sum of the squared values of the weights to the loss function. This discourages large weight values and leads to smoother, more stable models without eliminating features entirely.\n",
    "\n",
    "**L1 (Lasso):**\n",
    "$$ \\lambda \\sum |w| $$\n",
    "→ Produces sparse models\n",
    "\n",
    "**L2 (Ridge):**\n",
    "$$ \\lambda \\sum w^2 $$\n",
    "→ Penalizes large weights\n",
    "\n",
    "\n",
    "Both methods reduce overfitting by preventing the model from relying too heavily on any single feature.\n",
    "\n",
    "Let the original loss function be:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{original}} = \\frac{1}{m}\\sum_{i=1}^{m} \\ell(y_i, \\hat{y}_i)\n",
    "$$\n",
    "\n",
    "**L1 Regularization (Lasso):**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{L1}} = \\mathcal{L}_{\\text{original}} + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "$$\n",
    "\n",
    "**L2 Regularization (Ridge):**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{L2}} = \\mathcal{L}_{\\text{original}} + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361782e6",
   "metadata": {},
   "source": [
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout is a regularization technique in which a fraction of neurons is randomly deactivated during each training iteration. This forces the network to learn redundant representations and prevents neurons from becoming overly dependent on one another.\n",
    "\n",
    "By randomly removing neurons during training, dropout simulates training an ensemble of smaller networks. During inference, all neurons are active, and their outputs are appropriately scaled.\n",
    "\n",
    "Dropout significantly improves robustness and generalization, particularly in deep neural networks.\n",
    "\n",
    "\n",
    "For neuron activation $h$:\n",
    "\n",
    "$$\n",
    "r \\sim \\text{Bernoulli}(p)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{h} = r \\cdot h\n",
    "$$\n",
    "\n",
    "**Inverted Dropout:**\n",
    "$$\n",
    "\\tilde{h} = \\frac{r}{p} \\cdot h\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88534b64",
   "metadata": {},
   "source": [
    "\n",
    "## Data Augmentation (Introductory)\n",
    "\n",
    "Data augmentation is a regularization strategy that increases the effective size of the training dataset by creating modified versions of existing data samples. These modifications preserve the original label while introducing variation.\n",
    "\n",
    "Common augmentation techniques include geometric transformations, noise injection, and scaling. Data augmentation exposes the model to a wider range of input patterns, reducing overfitting and improving generalization.\n",
    "\n",
    "This technique is especially useful when the available training data is limited.\n",
    "\n",
    "\n",
    "Given a training sample:\n",
    "$$\n",
    "(x, y)\n",
    "$$\n",
    "\n",
    "Augmented sample:\n",
    "$$\n",
    "(\\tilde{x}, y), \\quad \\tilde{x} = T(x)\n",
    "$$\n",
    "\n",
    "Noise injection:\n",
    "$$\n",
    "\\tilde{x} = x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8a2c9",
   "metadata": {},
   "source": [
    "\n",
    "## Early Stopping\n",
    "\n",
    "Early stopping is a regularization technique that halts training when model performance on a validation dataset stops improving. While training loss may continue to decrease, validation performance may begin to degrade, signaling the onset of overfitting.\n",
    "\n",
    "By stopping training at the optimal point, early stopping prevents the model from memorizing the training data and reduces unnecessary computational cost.\n",
    "\n",
    "Early stopping is simple to implement and is widely used in practical neural network training.\n",
    "\n",
    "\n",
    "Validation loss at epoch $t$:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{val}}(t)\n",
    "$$\n",
    "\n",
    "Training stops when:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{val}}(t+k) > \\mathcal{L}_{\\text{val}}(t)\n",
    "$$\n",
    "\n",
    "Optimal epoch:\n",
    "$$\n",
    "t^* = \\arg\\min_t \\mathcal{L}_{\\text{val}}(t)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589eab3",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "- Data preparation\n",
    "- Model design\n",
    "- Forward pass & prediction\n",
    "- Evaluation using accuracy\n",
    "- Overfitting vs underfitting\n",
    "- Regularization techniques\n",
    "\n",
    "These ideas form the **foundation of deep learning training workflows**.\n",
    "\n",
    "\n",
    "\n",
    "## Task for the Reader\n",
    "\n",
    "1.Explain the role of data preprocessing in training a classification model.\n",
    "\n",
    "2.Design a simple MLP architecture for a binary classification problem.\n",
    "\n",
    "3.Compute the forward pass output for a given set of weights and inputs.\n",
    "\n",
    "4.Calculate model accuracy manually using true and predicted labels.\n",
    "\n",
    "5.Identify whether a given model is underfitting or overfitting from its behavior.\n",
    "\n",
    "6.Compare L1 and L2 regularization in terms of their effect on model weights.\n",
    "\n",
    "7.Explain how dropout helps in reducing overfitting.\n",
    "\n",
    "8.Give examples of data augmentation techniques for classification tasks.\n",
    "\n",
    "9.Describe the early stopping criterion using validation loss."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
