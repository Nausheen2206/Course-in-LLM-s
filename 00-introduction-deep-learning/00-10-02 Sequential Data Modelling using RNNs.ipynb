{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8509626d-6d8d-485a-bd71-a097b63ff899",
   "metadata": {},
   "source": [
    "# Sequential Data Modeling using Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In many real-world problems, data does not exist in isolation.  \n",
    "Instead, it arrives **as a sequence**, where the meaning of the current input depends on what came before it.\n",
    "\n",
    "Examples include:\n",
    "- Words in a sentence\n",
    "- Daily temperature readings\n",
    "- Stock prices over time\n",
    "- Sensor signals\n",
    "\n",
    "Traditional feed-forward neural networks process inputs independently and therefore **fail to capture these dependencies**.\n",
    "\n",
    "To handle such data, we use **Recurrent Neural Networks (RNNs)** — models designed to work with sequential information by maintaining an internal memory of past inputs.\n",
    "\n",
    "This notebook introduces the core idea of **sequential data modeling** and explains how RNNs learn patterns that unfold over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89680b-0012-43e9-9d0f-d7d00596e8e0",
   "metadata": {},
   "source": [
    "## Sequential Data and the Need for Specialized Models\n",
    "\n",
    "In many practical machine learning problems, the data is naturally ordered, and this order carries meaning. Such data is known as sequential data. In sequential data, each element is connected to the previous ones, and understanding the current element often requires knowledge of what came before it. This dependency on past information is what distinguishes sequential data from standard fixed-size inputs.\n",
    "\n",
    "Consider a sentence in natural language. The meaning of a word depends heavily on the words that precede it. Similarly, in time-series data such as temperature readings or stock prices, the current value is influenced by earlier observations. Treating each input independently in these cases leads to a loss of context and results in poor modeling of the underlying pattern.\n",
    "\n",
    "Traditional feed-forward neural networks are not designed to handle this type of dependency. They process each input in isolation and do not retain any information once the output is produced. As a result, they are unable to capture temporal relationships or long-term dependencies present in sequential data.\n",
    "\n",
    "To address this limitation, models are required that can remember past inputs while processing new ones. Recurrent Neural Networks were developed specifically for this purpose. They introduce the concept of a hidden state, which acts as a memory that is updated at each step of the sequence. This allows the network to incorporate information from previous time steps when making predictions, making RNNs suitable for sequential data modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130c191-fe5b-460c-98c6-f549f5d936c4",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "Recurrent Neural Networks are a class of neural networks specifically designed to model sequential data. Unlike feed-forward neural networks, which assume that all inputs are independent of each other, RNNs are built on the idea that past information can influence the processing of current input. This makes them suitable for problems where the order of data matters.\n",
    "\n",
    "The defining feature of an RNN is its ability to maintain an internal memory, known as the hidden state. When a sequence is processed, the network takes one element at a time and updates this hidden state at every step. The hidden state acts as a summary of all the information the network has seen so far in the sequence. As new inputs arrive, this memory is continuously updated rather than discarded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e180535-7be3-431b-8666-dda755b28694",
   "metadata": {},
   "source": [
    "## How Recurrent Neural Networks Work \n",
    "\n",
    "\n",
    "**Step 1: Learning from Data Like Traditional Neural Networks**\n",
    "\n",
    "Recurrent Neural Networks, like feedforward neural networks and convolutional neural networks, learn patterns from training data. They rely on forward propagation to generate outputs and use gradient-based optimization techniques to adjust their weights during training. In this sense, RNNs follow the same learning principles as other neural network architectures.\n",
    "\n",
    "**Step 2: Introducing Memory into the Network**\n",
    "\n",
    "The key difference between recurrent neural networks and traditional networks is the presence of memory. While feedforward and convolutional networks assume that all inputs are independent, RNNs are designed to work with data where previous inputs influence the current output. This memory allows RNNs to capture dependencies that unfold over time.\n",
    "\n",
    "**Step 3: Processing Data as a Sequence**\n",
    "\n",
    "RNNs process data one element at a time rather than all at once. Each element in the input sequence is handled in a specific order. The output at any given time step depends not only on the current input but also on information from earlier elements in the sequence. This makes RNNs suitable for tasks involving text, speech, and time-series data.\n",
    "\n",
    "**Step 4: Understanding the Role of the Hidden State**\n",
    "\n",
    "At the heart of an RNN is the hidden state. The hidden state acts as a memory that stores information about what the network has seen so far. After processing an input at one time step, the hidden state is passed to the next time step. This creates a feedback loop that allows contextual information to flow through the sequence.\n",
    "\n",
    "**Step 5: Combining Current Input with Past Information**\n",
    "\n",
    "At each time step, the RNN takes two inputs: the current data point and the hidden state from the previous time step. These inputs are combined using shared weights and passed through an activation function to produce a new hidden state. This updated hidden state reflects both the current input and the accumulated context from earlier inputs.\n",
    "\n",
    "**Step 6: Preserving Word Order and Context**\n",
    "\n",
    "To understand why order matters, consider the phrase “feeling under the weather.” The meaning of this idiom depends entirely on the words appearing in a specific sequence. An RNN processes each word sequentially and uses its hidden state to remember earlier words, allowing it to correctly interpret or predict the next word in the phrase.\n",
    "\n",
    "**Step 7: Sharing Parameters Across Time Steps**\n",
    "\n",
    "Another defining feature of RNNs is parameter sharing. The same set of weights is used at every time step of the sequence. Unlike feedforward networks, where each layer has different weights, RNNs reuse the same weights repeatedly. This enables the network to generalize across sequences of different lengths while keeping the model efficient.\n",
    "\n",
    "**Step 8: Training Using Backpropagation Through Time**\n",
    "\n",
    "Recurrent Neural Networks are trained using a technique called Backpropagation Through Time (BPTT). This method is an extension of traditional backpropagation. Because the same parameters are used across multiple time steps, errors are computed at each time step and then summed before updating the weights. This allows the network to learn how earlier inputs influence later outputs in the sequence.\n",
    "\n",
    "![](https://assets.ibm.com/is/image/ibm/what-are-recurrent-neural-networks-combined:1x1?dpr=on%2C1.25&wid=512&hei=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145a5a3-6cdb-469a-9817-2217e4430c3a",
   "metadata": {},
   "source": [
    "## Activation Functions in RNNs\n",
    "\n",
    "Activation functions control how information flows through a recurrent neural network and how the hidden state is updated at each time step. The choice of activation function affects gradient stability and the model’s ability to learn long-term dependencies.\n",
    "\n",
    "The **Sigmoid function** is commonly used when outputs need to be interpreted as probabilities or when controlling information flow, such as in gating mechanisms. However, sigmoid is prone to the vanishing gradient problem, which limits its effectiveness for long sequences.\n",
    "\n",
    "The **Tanh (hyperbolic tangent) function** is often preferred in RNNs because its outputs are centered around zero. This improves gradient flow and makes learning long-term dependencies easier compared to sigmoid.\n",
    "\n",
    "The **ReLU activation function** allows stronger gradient flow for positive inputs but can lead to exploding gradients due to its unbounded nature. To reduce this risk, variants such as Leaky ReLU and Parametric ReLU are sometimes used in recurrent models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a6b5f2-a5a8-456c-bfaf-1c9d69fb3ce9",
   "metadata": {},
   "source": [
    "## Example: Sequential Data Modeling with a Simple RNN\n",
    "We will build a simple RNN that takes a sequence of numbers as input and predicts a single output class.\n",
    "This mirrors real-world tasks such as sentiment analysis or time-series classification, where the final decision depends on the entire sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ae0f2d-9464-4ff4-b697-f95b6b5525be",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5ac039-6ee5-4baa-8681-9b697eee0304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc79e9-a7f3-4301-9145-f95101cc677d",
   "metadata": {},
   "source": [
    "### Step 2: Create a Toy Sequential Dataset\n",
    "\n",
    "Each input is a sequence of length 5, and the label depends on the overall pattern of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0e70b3-951b-47ea-a7ff-8e062b690775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: batch_size x sequence_length x input_size\n",
    "X = torch.tensor([\n",
    "    [[1.0], [2.0], [3.0], [4.0], [5.0]],\n",
    "    [[5.0], [4.0], [3.0], [2.0], [1.0]]\n",
    "])\n",
    "\n",
    "# Output labels (many-to-one)\n",
    "y = torch.tensor([1, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c245f19-87b9-42dc-a73b-2fb43dfa30ab",
   "metadata": {},
   "source": [
    "### Step 3: Define the RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a257c-bc9b-4d0b-9f78-ea99fff028fb",
   "metadata": {},
   "source": [
    "\n",
    "In this step, we define the architecture of our Recurrent Neural Network. The model consists of two main components: a recurrent layer that processes sequential data and a fully connected layer that produces the final output.\n",
    "\n",
    "We define the model by creating a class that inherits from `nn.Module`. This is a standard practice in PyTorch and allows the model to automatically track parameters and gradients during training.\n",
    "\n",
    "The constructor of the class initializes the layers used in the network. The RNN layer is responsible for processing the input sequence one time step at a time. It takes three important parameters: the size of each input element, the size of the hidden state, and whether the input data is provided in batch-first format. Setting `batch_first=True` means the input tensor is expected in the shape `(batch_size, sequence_length, input_size)`, which is intuitive and commonly used.\n",
    "\n",
    "The recurrent layer outputs two values. The first is a sequence of hidden states, one for each time step. The second is the final hidden state of the sequence. In this model, we focus on the hidden states produced at each time step rather than directly using the final hidden state.\n",
    "\n",
    "After the RNN layer, a fully connected (linear) layer is defined. This layer maps the hidden representation produced by the RNN to the desired output size. In a sequence classification task, this layer converts the learned temporal representation into class scores.\n",
    "\n",
    "The forward method defines how data flows through the model. When an input sequence is passed to the RNN, it is processed sequentially, and hidden states are generated for each time step. From this sequence of hidden states, only the hidden state corresponding to the final time step is selected. This final hidden state contains information accumulated from the entire sequence and is therefore suitable for making a prediction.\n",
    "\n",
    "The selected hidden state is then passed through the fully connected layer to produce the final output. This output represents the model’s prediction based on the full sequence rather than any single input element.\n",
    "\n",
    "Overall, this model follows a many-to-one architecture, where a sequence of inputs is mapped to a single output. The recurrent layer captures temporal dependencies, while the fully connected layer translates those learned dependencies into a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5033a3e8-94e3-4cdc-b563-068e28a9246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x)\n",
    "        last_hidden = out[:, -1, :]\n",
    "        output = self.fc(last_hidden)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb5073-48ce-4581-b28a-1b1d25df18c0",
   "metadata": {},
   "source": [
    "### Step 4: Initialize Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afbc979-6af3-4762-b6d9-1fe010e8931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(input_size=1, hidden_size=8, output_size=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed361380-e168-4a68-b42f-af2bcdbbfe2a",
   "metadata": {},
   "source": [
    "### Step 5: Train the RNN\n",
    "During training, the model repeatedly processes the input sequences, compares its predictions with the true labels, and adjusts its internal parameters to reduce prediction errors.\n",
    "\n",
    "The training loop runs for a fixed number of epochs. An epoch represents one complete pass through the training dataset. Multiple epochs are required because neural networks do not learn optimal parameters in a single pass; instead, they gradually improve through repeated exposure to the data.\n",
    "\n",
    "At the beginning of each epoch, the gradients stored from the previous iteration are cleared. This is necessary because PyTorch accumulates gradients by default. If gradients are not reset, updates from earlier epochs would incorrectly influence the current update step.\n",
    "\n",
    "Next, the input sequences are passed through the model using a forward pass. During this step, the RNN processes the sequence one time step at a time, updating its hidden state internally and finally producing an output based on the last hidden state. This output represents the model’s current prediction for the entire sequence.\n",
    "\n",
    "The predicted output is then compared with the true labels using a loss function. The loss function quantifies how far the model’s predictions are from the correct answers. A lower loss indicates better performance, while a higher loss indicates larger prediction errors.\n",
    "\n",
    "Once the loss is computed, backpropagation is performed by calling the backward function. In recurrent neural networks, this step internally applies Backpropagation Through Time. Errors are propagated backward across all time steps of the sequence, allowing the shared weights of the RNN to be updated based on their contribution to the final error.\n",
    "\n",
    "After gradients are computed, the optimizer updates the model’s parameters. The optimizer uses the gradients and the learning rate to make small adjustments to the weights, with the goal of reducing the loss in the next iteration.\n",
    "\n",
    "This process of forward pass, loss computation, backward pass, and parameter update is repeated for every epoch. Over time, the RNN learns to capture sequential patterns and improves its predictions by effectively using information from earlier time steps in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b950dc3-2d2a-4ab9-8298-13cb12f3c6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/200], Loss: 0.0127\n",
      "Epoch [100/200], Loss: 0.0033\n",
      "Epoch [150/200], Loss: 0.0018\n",
      "Epoch [200/200], Loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/200], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a598c-bf48-4018-87e4-a9a75bec5058",
   "metadata": {},
   "source": [
    "### Step 6: Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf046c85-9521-4cf9-b0dc-91664852f4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = torch.argmax(model(X), dim=1)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41dfcd-3a21-49ae-9b70-c68d788bf521",
   "metadata": {},
   "source": [
    "When the model is tested, it produces a tensor of values for each input sequence. These values are called logits. Each logit corresponds to a class, and larger values indicate higher confidence. However, logits themselves are not probabilities and should not be interpreted directly as predictions.\n",
    "\n",
    "To convert these logits into a final prediction, we apply an operation such as `argmax`. This selects the index of the largest value in the output tensor, which corresponds to the class the model believes is most likely. The resulting output is a class label rather than raw scores.\n",
    "\n",
    "Even after converting logits into class labels, the output alone does not explain how the model arrived at its decision. At this stage, the model is effectively acting as a black box. It has learned patterns across the sequence, but those patterns are embedded within its hidden states and weight parameters.\n",
    "\n",
    "To better understand and validate the model’s behavior, several steps can be taken after testing. One approach is to compare the predicted labels with the true labels to measure accuracy. Another is to test the model on new, unseen sequences to evaluate how well it generalizes. Inspecting the loss value during training can also help determine whether the model has learned meaningful patterns or is underfitting or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a705f927-3603-4151-a615-ca45545c7ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "correct = (predictions == y).sum().item()\n",
    "accuracy = correct / y.size(0)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb32c42-29fe-4313-b1ad-26f657a6276b",
   "metadata": {},
   "source": [
    "The printed accuracy value of 1.0 indicates that the model has correctly predicted all labels in the test data. In numerical terms, this means that every predicted class label exactly matches the corresponding true label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78c93d-2430-4ca6-a443-210fce23b6d0",
   "metadata": {},
   "source": [
    "## Experiment: Does the RNN Really Learn Sequence Order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a82edf-ef30-478a-9a71-aa386c6844e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for original sequence: 1\n",
      "Prediction for reversed sequence: 0\n"
     ]
    }
   ],
   "source": [
    "# Original sequence\n",
    "original_sequence = torch.tensor([[[1.0], [2.0], [3.0], [4.0], [5.0]]])\n",
    "\n",
    "# Reversed sequence\n",
    "reversed_sequence = torch.tensor([[[5.0], [4.0], [3.0], [2.0], [1.0]]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_pred = torch.argmax(model(original_sequence), dim=1)\n",
    "    reversed_pred = torch.argmax(model(reversed_sequence), dim=1)\n",
    "\n",
    "print(\"Prediction for original sequence:\", original_pred.item())\n",
    "print(\"Prediction for reversed sequence:\", reversed_pred.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513b555-731a-4663-9d5f-6d135ffb0bf4",
   "metadata": {},
   "source": [
    "## Verifying Sequential Learning Through Order Sensitivity\n",
    "\n",
    "To confirm that the recurrent neural network is truly modeling sequential data, we perform a simple but powerful experiment. Instead of changing the values in the input, we change only the order in which those values appear. This allows us to test whether the model is sensitive to sequence order.\n",
    "\n",
    "The original and reversed sequences contain the same elements, but arranged differently. If the model were treating inputs independently, changing the order would not significantly affect the prediction. However, in a properly functioning RNN, the order of inputs plays a critical role in shaping the hidden state at each time step.\n",
    "\n",
    "When the model processes the original sequence, the hidden state evolves in a specific manner as information flows from earlier to later time steps. Reversing the sequence changes this flow entirely, leading to a different final hidden state and, consequently, a different prediction.\n",
    "\n",
    "If the predictions for the original and reversed sequences are different, it confirms that the RNN is learning and using temporal dependencies rather than relying solely on the input values. This behavior is a defining characteristic of sequential data modeling.\n",
    "\n",
    "This experiment demonstrates that the RNN does not simply memorize values but instead learns how information accumulates over time. Such sensitivity to order is essential for tasks like language modeling, speech recognition, and time-series forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9c3a3-67bd-4fdb-8472-63474cd8229e",
   "metadata": {},
   "source": [
    "## Task for the Reader\n",
    "\n",
    "1. Modify the input sequences by increasing their length and observe how the model’s predictions change. Analyze whether the RNN is still able to capture meaningful patterns as the sequence becomes longer.\n",
    "\n",
    "2. Shuffle the order of elements within a sequence and compare the predictions with those obtained from the original sequence. Explain how and why the change in order affects the output.\n",
    "\n",
    "3. Print and examine the hidden state at each time step for a given input sequence. Describe how the hidden state evolves as new elements are processed and how it reflects accumulated sequence information.\n",
    "\n",
    "4. Replace the RNN activation function with a different one, such as ReLU or Tanh, and observe its effect on training stability and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
