{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297f60fe-6344-40a8-8771-767c7b6c829b",
   "metadata": {},
   "source": [
    "# LSTM \n",
    "\n",
    "When we talk about a Long Short-Term Memory (LSTM) network, we are referring to a\n",
    "special type of recurrent neural network cell that is designed to handle memory\n",
    "in a more effective and controlled way.\n",
    "\n",
    "In a traditional RNN, memory is stored in a single hidden state that gets updated\n",
    "at every time step. This makes the network prone to forgetting important\n",
    "information from earlier in the sequence, especially when dealing with long\n",
    "inputs.\n",
    "\n",
    "The LSTM cell addresses this limitation by introducing a structured memory\n",
    "mechanism that allows the network to retain important information over long\n",
    "periods of time while discarding irrelevant details.\n",
    "\n",
    "The key idea behind the LSTM cell is the separation of memory into two parts:\n",
    "a long-term memory called the *cell state* and a short-term memory called the\n",
    "*hidden state*. This separation allows the network to preserve important\n",
    "information while still producing useful outputs at each time step.\n",
    "\n",
    "To control how information flows into, out of, and through the cell state, the\n",
    "LSTM uses gating mechanisms. These gates act like filters that decide what\n",
    "information should be remembered, forgotten, or exposed as output. Because these\n",
    "decisions are made using smooth, learnable functions, the LSTM can be trained\n",
    "effectively using gradient-based optimization methods.\n",
    "\n",
    "Overall, the LSTM cell is designed to remember what is important, forget what is\n",
    "not, and maintain stable learning across long sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fde8d4-2af5-4c7b-9262-d7b0903e8628",
   "metadata": {},
   "source": [
    "## The Cell State:\n",
    "\n",
    "The most important component of an LSTM cell is the **cell state**. This is where\n",
    "long-term information is stored. Unlike the hidden state in a traditional RNN,\n",
    "the cell state is designed to change very slowly over time.\n",
    "\n",
    "You can think of the cell state as a memory line that runs straight through the\n",
    "entire sequence. Information can flow along this line with very little\n",
    "interference, which allows important details from earlier time steps to be\n",
    "preserved for a long duration.\n",
    "\n",
    "What makes the cell state special is that it is not repeatedly overwritten.\n",
    "Instead, it is updated in a controlled manner using gating mechanisms. These\n",
    "gates decide which parts of the previous memory should be kept and which parts\n",
    "should be modified.\n",
    "\n",
    "Because the cell state follows a mostly linear path through time, gradients can\n",
    "flow backward during training without shrinking too much. This directly helps\n",
    "solve the vanishing gradient problem that affects traditional recurrent neural\n",
    "networks.\n",
    "\n",
    "In simple terms, the cell state acts as the long-term memory of the LSTM. It\n",
    "stores information that is relevant across many time steps and provides a stable\n",
    "foundation on which the rest of the LSTM operations are built.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6081a3-f803-4c29-85b2-ba9280d4da58",
   "metadata": {},
   "source": [
    "## The Forget Gate: Deciding What to Remove from Memory\n",
    "\n",
    "As an LSTM processes a sequence, not all past information remains useful forever.\n",
    "Some details become irrelevant as new inputs arrive. The purpose of the forget\n",
    "gate is to decide which parts of the existing memory should be discarded.\n",
    "\n",
    "At each time step, the forget gate looks at two things: the current input and the\n",
    "previous hidden state. Based on these, it produces a set of values between 0 and 1 . These values act as control signals for the cell state.\n",
    "\n",
    "A value close to 1 means that the corresponding information in the cell state\n",
    "should be kept almost unchanged. A value close to 0 means that the information\n",
    "should be largely forgotten. In this way, the forget gate selectively cleans the\n",
    "memory.\n",
    "\n",
    "This mechanism is crucial because memory that is never updated or cleaned can\n",
    "become noisy and harmful to learning. By allowing the network to forget\n",
    "irrelevant information, the LSTM maintains a meaningful and focused long-term\n",
    "memory.\n",
    "\n",
    "In essence, the forget gate ensures that the cell state contains only information\n",
    "that continues to be useful for understanding the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d0811-330f-4c4e-8465-f6613cb9603f",
   "metadata": {},
   "source": [
    "## The Input Gate: Deciding What New Information to Store\n",
    "\n",
    "After deciding what information to remove from memory, the LSTM must determine\n",
    "what new information is important enough to be stored. This responsibility lies\n",
    "with the input gate.\n",
    "\n",
    "The input gate controls how much new information should be written into the cell\n",
    "state. It does not directly decide the content of the information, but rather how\n",
    "strongly new information should influence the memory.\n",
    "\n",
    "At the same time, the LSTM creates a set of candidate values that represent new\n",
    "information derived from the current input and the previous hidden state. These\n",
    "candidate values describe what could potentially be added to the memory.\n",
    "\n",
    "The input gate then acts as a filter on this candidate information. Only the\n",
    "portions that are considered important are allowed to enter the cell state,\n",
    "while the rest are suppressed.\n",
    "\n",
    "This two-step process ensures that the LSTM does not blindly store everything it\n",
    "encounters. Instead, it carefully selects and integrates only relevant new\n",
    "information, helping the network maintain a meaningful and efficient long-term\n",
    "memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657aca59-bf75-4a13-aa62-ee4caa1278c0",
   "metadata": {},
   "source": [
    "## Updating the Cell State: Combining Old and New Memory\n",
    "\n",
    "Once the LSTM has decided what information to forget and what new information to\n",
    "store, it updates the cell state. This step is the heart of the LSTM mechanism\n",
    "because it determines how memory evolves over time.\n",
    "\n",
    "The update process combines two sources of information. First, the existing cell\n",
    "state is adjusted by removing the parts marked as unimportant by the forget\n",
    "gate. Second, new information selected by the input gate is added to the memory.\n",
    "\n",
    "This update happens in a smooth and controlled way rather than through abrupt\n",
    "overwriting. As a result, important information can persist across many time\n",
    "steps while still allowing the memory to adapt when needed.\n",
    "\n",
    "Because the cell state follows a mostly linear path through time, this update\n",
    "structure allows gradients to flow backward more effectively during training.\n",
    "This is a key reason why LSTM networks are able to learn long-term dependencies\n",
    "that traditional recurrent neural networks struggle with.\n",
    "\n",
    "In essence, updating the cell state is what allows the LSTM to balance stability\n",
    "and flexibility in its memory, remembering what matters while remaining open to\n",
    "new information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65b145-0cbb-40c1-a156-3780331d7d37",
   "metadata": {},
   "source": [
    "## The Output Gate: Controlling What the LSTM Reveals\n",
    "\n",
    "After updating its internal memory, the LSTM must decide what information should\n",
    "be exposed as output at the current time step. This decision is handled by the\n",
    "output gate.\n",
    "\n",
    "The output gate examines the current input and the previous hidden state to\n",
    "determine which parts of the updated cell state are relevant for the present\n",
    "moment. Not all information stored in memory needs to be visible at every step.\n",
    "\n",
    "To produce the output, the cell state is first transformed into a scaled form so\n",
    "that its values remain within a manageable range. The output gate then filters\n",
    "this transformed memory, allowing only the most relevant information to pass\n",
    "through.\n",
    "\n",
    "The result of this process is the new hidden state. This hidden state serves two\n",
    "important purposes: it is passed to the next time step in the sequence, and it is\n",
    "used to generate predictions or outputs for the current step.\n",
    "\n",
    "In simple terms, the output gate ensures that the LSTM shares only what is\n",
    "necessary, keeping the rest of the memory protected for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b284c-e8b8-4247-8049-f87adcb5765d",
   "metadata": {},
   "source": [
    "![](https://d2l.ai/_images/lstm-0.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c892bc-81ba-4427-95a8-31e5ea9493d1",
   "metadata": {},
   "source": [
    "## Example\n",
    "In this example, we manually implement the internal computations of an LSTM cell\n",
    "to understand how it processes sequential data, updates memory, and produces\n",
    "outputs at each time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf7f48-8cc8-4d45-9ac6-067089088d9f",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4ba47b-c576-482f-8f2b-a2b607ccbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30af4d-0a04-481e-87d6-01ba4b4bd9fe",
   "metadata": {},
   "source": [
    "### Step 2: Initialize LSTM Parameters\n",
    "This cell defines a function that creates all trainable parameters of the LSTM\n",
    "cell. It includes weight matrices and bias vectors for the forget gate, input\n",
    "gate, candidate memory, and output gate.\n",
    "\n",
    "Each gate has:\n",
    "\n",
    "input-to-hidden weights,\n",
    "\n",
    "hidden-to-hidden weights,\n",
    "\n",
    "a bias term.\n",
    "\n",
    "Weights are initialized with small random values to ensure numerical stability.\n",
    "All parameters are marked to require gradients so that they can be updated during\n",
    "training. The function returns all parameters as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83adcb9b-5ff3-4bb2-a934-71051ef1740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                torch.zeros(num_hiddens, device=device))\n",
    "\n",
    "    # Forget gate\n",
    "    W_xf, W_hf, b_f = three()\n",
    "    # Input gate\n",
    "    W_xi, W_hi, b_i = three()\n",
    "    # Candidate memory\n",
    "    W_xc, W_hc, b_c = three()\n",
    "    # Output gate\n",
    "    W_xo, W_ho, b_o = three()\n",
    "\n",
    "    # Output layer\n",
    "    W_hq = normal((num_hiddens, vocab_size))\n",
    "    b_q = torch.zeros(vocab_size, device=device)\n",
    "\n",
    "    params = [W_xf, W_hf, b_f,\n",
    "              W_xi, W_hi, b_i,\n",
    "              W_xc, W_hc, b_c,\n",
    "              W_xo, W_ho, b_o,\n",
    "              W_hq, b_q]\n",
    "\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b40509-56e7-4a1e-853b-62644929e846",
   "metadata": {},
   "source": [
    "###  Step 3 Initialize Hidden State and Cell State\n",
    "This cell defines a function that initializes the hidden state and cell\n",
    "state of the LSTM. Both are set to zero at the start of a sequence, indicating\n",
    "that the model begins with no prior memory.\n",
    "\n",
    "The hidden state represents short-term information, while the cell state stores\n",
    "long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33bbb67-8023-4d9a-9689-9daf5aed30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),\n",
    "            torch.zeros((batch_size, num_hiddens), device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8724a-32d1-4745-b49f-00c98692be39",
   "metadata": {},
   "source": [
    "### Step 4 LSTM Forward Computation\n",
    "This cell contains the core logic of the LSTM. It processes the input sequence one\n",
    "time step at a time and updates the hidden and cell states using LSTM gate\n",
    "operations.\n",
    "\n",
    "At each time step:\n",
    "\n",
    "- the forget gate controls what old memory is kept,\n",
    "\n",
    "- the input gate selects new information to add,\n",
    "\n",
    "- the cell state is updated,\n",
    "\n",
    "the output gate determines what information is exposed.\n",
    "\n",
    "The hidden state is then transformed into an output vector. Outputs from all time\n",
    "steps are collected and returned along with the final states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90162813-3dcf-49a3-a89d-12b0f99c1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs, state, params):\n",
    "    (H, C) = state\n",
    "    (W_xf, W_hf, b_f,\n",
    "     W_xi, W_hi, b_i,\n",
    "     W_xc, W_hc, b_c,\n",
    "     W_xo, W_ho, b_o,\n",
    "     W_hq, b_q) = params\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for X in inputs:\n",
    "        F = torch.sigmoid(X @ W_xf + H @ W_hf + b_f)\n",
    "        I = torch.sigmoid(X @ W_xi + H @ W_hi + b_i)\n",
    "        C_tilde = torch.tanh(X @ W_xc + H @ W_hc + b_c)\n",
    "        C = F * C + I * C_tilde\n",
    "        O = torch.sigmoid(X @ W_xo + H @ W_ho + b_o)\n",
    "        H = O * torch.tanh(C)\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "\n",
    "    return torch.cat(outputs, dim=0), (H, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826692e-46ac-4bd8-9c7a-9d3c88b30a12",
   "metadata": {},
   "source": [
    "### Step 5 Test the Implementation\n",
    "This cell tests the LSTM implementation using dummy input data. It initializes the\n",
    "parameters and memory states, creates a random input sequence, and passes it\n",
    "through the LSTM function.\n",
    "\n",
    "The purpose of this cell is to verify that the LSTM runs correctly and produces\n",
    "outputs with the expected shape. The model is not trained here; this step only\n",
    "confirms that the implementation is logically and syntactically correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a828775e-6693-4968-9098-f939a5228af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "vocab_size = 10\n",
    "num_hiddens = 16\n",
    "batch_size = 2\n",
    "num_steps = 5\n",
    "\n",
    "params = get_lstm_params(vocab_size, num_hiddens, device)\n",
    "state = init_lstm_state(batch_size, num_hiddens, device)\n",
    "\n",
    "inputs = torch.randn(num_steps, batch_size, vocab_size)\n",
    "\n",
    "outputs, state = lstm(inputs, state, params)\n",
    "\n",
    "outputs.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107f937e-b173-4c28-a365-6f71ae00abd2",
   "metadata": {},
   "source": [
    "The output is produced at every time step from the hidden state of the LSTM.\n",
    "Its shape reflects the sequence length and batch size, while its values appear\n",
    "random because the model has not been trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498accce-4303-42e4-9d24-b5cb7523f949",
   "metadata": {},
   "source": [
    "## Task for the reader\n",
    "Train the LSTM on the same task using different sequence lengths. Analyze how increasing or decreasing the sequence length affects learning and output quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
