{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e520e76d-2d82-4fcb-a82d-2c3ec6f3a567",
   "metadata": {},
   "source": [
    "# Data Handling and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5eda1-a645-4a47-8da2-5994630783a2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The quality of your data directly impacts the accuracy of your analysis and model performance.\n",
    "Because raw data often contains inconsistencies, errors, and irrelevant information that can distort results and lead to flawed insights. Data preprocessing is a way to mitigate this problem. \n",
    "\n",
    "**Data preprocessing** is a key aspect of data preparation. It refers to any processing applied to raw data to ready it for further analysis or processing tasks. \n",
    "\n",
    "Traditionally, data preprocessing has been an essential preliminary step in data analysis. However, more recently, these techniques have been adapted to train machine learning and AI models and make inferences from them. \n",
    "\n",
    "Thus, data preprocessing may be defined as the process of converting raw data into a format that can be processed more efficiently and accurately in tasks such as: \n",
    "\n",
    "- Data analysis\n",
    "- Machine learning \n",
    "- Data science\n",
    "- AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe1d34-4f59-469a-b04a-3ac9dd7dc2d6",
   "metadata": {},
   "source": [
    "##  Purpose of Data Splitting\n",
    "\n",
    "Data splitting is the process of dividing a dataset into separate subsets for training,\n",
    "validation, and testing.\n",
    "\n",
    "### Why is data splitting important?\n",
    "- To evaluate model performance on unseen data\n",
    "- To prevent overfitting\n",
    "- To tune hyperparameters correctly\n",
    "- To simulate real-world deployment scenarios\n",
    "\n",
    "Without proper splitting, models may memorize data instead of learning patterns.\n",
    "\n",
    "## Typical Steps in Data Preprocessing\n",
    "\n",
    "The following sequence outlines a standard workflow :\n",
    "\n",
    "1. Acquire the dataset\n",
    "2. Import necessary libraries\n",
    "3. Load the dataset\n",
    "4. Explore and check for missing values\n",
    "5. Encode non-numerical (categorical) data\n",
    "6. Scale/normalize features\n",
    "7. Split data into training, validation, and test sets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa1e77-17b6-4637-bd0d-ed43d1c6c4e3",
   "metadata": {},
   "source": [
    "### Understanding Data Before Preprocessing\n",
    "\n",
    "Before performing any data handling or preprocessing steps, it is important to understand what the data represents. In machine learning, data is typically organized into **features** and a **target variable**.\n",
    "\n",
    "**Features** are the input variables that describe the data. They contain information that the model uses to learn patterns. Examples include numerical values such as age, height, attendance percentage, or hours studied.\n",
    "\n",
    "The **target variable** is the output we want the model to predict. Depending on the problem, it can represent a category (such as pass/fail) or a numerical value (such as a score or price).\n",
    "\n",
    "Data can also be classified based on its type:\n",
    "- **Numerical data** represents measurable quantities and can be continuous or discrete.\n",
    "- **Categorical data** represents labels or categories and usually needs to be converted into numerical form before modeling.\n",
    "\n",
    "Understanding the role and type of each column in the dataset helps in choosing the correct preprocessing techniques. This step ensures that the decisions made during data cleaning, encoding, and scaling are meaningful and appropriate for the problem being solved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a74172f-f568-419e-b01b-0841b165da58",
   "metadata": {},
   "source": [
    "## Step 1: Acquiring the Dataset\n",
    "\n",
    "The first step in any machine learning workflow is acquiring the dataset. In real-world projects, data is often collected from external sources such as databases, APIs, or public repositories. However, for learning and experimentation, creating a dataset manually is often a better starting point.\n",
    "\n",
    "In this step, we create our own dataset** instead of downloading one. This allows us to clearly understand what each feature represents and how the data is structured, without any hidden complexities. By designing the dataset ourselves, we have full control over the values and can easily relate them to real-world meaning.\n",
    "\n",
    "The dataset represents a simple student performance scenario:\n",
    "- `hours_studied` indicates the number of hours a student spent studying.\n",
    "- `attendance` represents the student’s attendance percentage.\n",
    "- `result` is the target variable that shows whether the student **passed** or **failed** the exam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b98e0e-e04b-41a2-845d-264242f4055c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours_studied</th>\n",
       "      <th>attendance</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>88</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>90</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours_studied  attendance result\n",
       "0              1          60   Fail\n",
       "1              2          65   Fail\n",
       "2              3          70   Fail\n",
       "3              4          72   Fail\n",
       "4              5          75   Fail\n",
       "5              6          80   Pass\n",
       "6              7          85   Pass\n",
       "7              8          88   Pass\n",
       "8              9          90   Pass\n",
       "9             10          95   Pass"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"hours_studied\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"attendance\": [60, 65, 70, 72, 75, 80, 85, 88, 90, 95],\n",
    "    \"result\": [\"Fail\", \"Fail\", \"Fail\", \"Fail\", \"Fail\",\n",
    "               \"Pass\", \"Pass\", \"Pass\", \"Pass\", \"Pass\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(\"student_data.csv\", index=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c834df-98f9-43aa-a82a-cd12764bf1bf",
   "metadata": {},
   "source": [
    "## Step 2: Importing the Required Libraries\n",
    "\n",
    "Once the dataset is ready, the next step is to import the necessary libraries that will help us work with the data. In Python, machine learning tasks rely heavily on a few powerful libraries that simplify data handling, preprocessing, and model development.\n",
    "\n",
    "In this notebook, we use:\n",
    "- **pandas**to handle and manipulate tabular data using DataFrames\n",
    "- **numpy** to perform numerical operations efficiently\n",
    "- **scikit-learn** to access tools for preprocessing and data splitting\n",
    "\n",
    "Importing these libraries at the beginning ensures that all required functionality is available as we move through the workflow. It also keeps the notebook organized and avoids repeated imports later.\n",
    "\n",
    "This step does not perform any computation on the data yet. It simply prepares the working environment so that we can load, explore, and preprocess the dataset smoothly in the following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9f9684-2c7a-419f-9b94-6e868cefe392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb486c3-76e3-4dd2-a548-b79bcd1ea8fd",
   "metadata": {},
   "source": [
    "## Step 3: Loading the Dataset\n",
    "\n",
    "After setting up the working environment, the next step is to load the dataset into the notebook. Since we created and saved our dataset as a CSV file in the previous step, we now read that file into memory so that it can be explored and processed.\n",
    "\n",
    "The dataset is loaded using the pandas library and stored in a DataFrame. A DataFrame organizes data in a tabular format with rows and columns, similar to a spreadsheet. This makes it easy to view, analyze, and manipulate the data.\n",
    "\n",
    "Once the dataset is loaded, it is good practice to display the first few rows. This helps us verify that the data has been loaded correctly and gives us a quick overview of the features, their values, and the overall structure of the dataset.\n",
    "\n",
    "At this stage, we are not modifying the data in any way. The goal is simply to make sure the dataset is available and ready for exploration and preprocessing in the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9495c72c-eb15-481a-9d60-c7ca32d0ff95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours_studied</th>\n",
       "      <th>attendance</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours_studied  attendance result\n",
       "0              1          60   Fail\n",
       "1              2          65   Fail\n",
       "2              3          70   Fail\n",
       "3              4          72   Fail\n",
       "4              5          75   Fail"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"student_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5ceb6-9142-4c93-8f7a-944c234e5346",
   "metadata": {},
   "source": [
    "## Step 4: Checking for Missing (Null) Values\n",
    "\n",
    "Before moving forward with preprocessing, it is important to check whether the dataset contains any missing or null values. Missing values can occur due to data entry errors, incomplete records, or issues during data collection. If not handled properly, they can lead to incorrect results or errors during model training.\n",
    "\n",
    "In this step, we examine each column of the dataset to identify whether any values are missing. This allows us to understand the completeness of the data and decide whether any cleaning action is required.\n",
    "\n",
    "If **missing values are found**, common approaches include:\n",
    "- Removing rows that contain missing values (when very few are missing)\n",
    "- Replacing missing values with a representative value such as the mean, median, or most frequent value\n",
    "- Using more advanced imputation techniques when necessary\n",
    "\n",
    "If **no missing values are found**, no cleaning action is required for this step. In such cases, the dataset is already complete, and we can confidently move on to the next preprocessing stage.\n",
    "\n",
    "For our manually created dataset, there are no missing values. This confirms that the data is clean and consistent, allowing us to proceed without performing any imputation or removal. Checking for missing values is still an essential step, even when the dataset appears simple, as it ensures data reliability and good preprocessing practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc9e70a7-8f62-41aa-aeea-44c80f20bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   hours_studied  10 non-null     int64 \n",
      " 1   attendance     10 non-null     int64 \n",
      " 2   result         10 non-null     object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 372.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hours_studied    0\n",
       "attendance       0\n",
       "result           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.info()\n",
    "\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0ed5b-c631-4b47-b61d-f7ef0e546ac1",
   "metadata": {},
   "source": [
    "In real-world datasets, missing or null values are very common. They may occur due to incomplete data collection, human error, or system issues. Before moving further in the preprocessing pipeline, it is important to check whether the dataset contains any missing values and decide how to handle them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc20e3c-2d47-458b-b6df-715bf67c4032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hours_studied    0\n",
       "attendance       0\n",
       "result           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe820d0-768e-44b5-b942-bb965d98e56a",
   "metadata": {},
   "source": [
    "### Approaches to Handling Missing Data\n",
    "In many real-world datasets, removing rows with missing values may lead to significant data loss, especially when the dataset is large or when missing values occur frequently. In such cases, alternative strategies are often preferred.\n",
    "\n",
    "One common approach is **imputation**, where missing values are replaced with a representative value. For numerical data, this could be the mean or median of the column. For categorical data, the most frequent value or a separate category such as “Unknown” may be used.\n",
    "\n",
    "Another important consideration is the importance of the feature. If a feature plays a critical role in the prediction task, preserving its data through imputation is often better than deleting records.\n",
    "\n",
    "The choice of how to handle missing values depends on:\n",
    "- The size of the dataset\n",
    "- The percentage of missing values\n",
    "- The importance of the affected feature\n",
    "- The type of machine learning model being used\n",
    "\n",
    "Understanding these options helps in making informed preprocessing decisions, even when a simple method like row removal is used for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41add94-5c47-45e7-adae-34904a59d247",
   "metadata": {},
   "source": [
    "## Step 5: Encoding Categorical Data\n",
    "\n",
    "Machine learning models require numerical input.\n",
    "Since the `result` column contains text values (\"Pass\" and \"Fail\"),\n",
    "we convert them into numerical labels.\n",
    "\n",
    "Label Encoding assigns:\n",
    "- `Fail` → 0\n",
    "- `Pass` → 1\n",
    "\n",
    "This transformation makes the target variable suitable for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd0abb7d-68cf-496a-9801-efcfe1f42a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours_studied</th>\n",
       "      <th>attendance</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours_studied  attendance  result\n",
       "0              1        60.0       0\n",
       "1              2        65.0       0\n",
       "2              3        70.0       0\n",
       "4              5        75.0       0\n",
       "5              6        80.0       1\n",
       "6              7        85.0       1\n",
       "7              8        88.0       1\n",
       "8              9        90.0       1\n",
       "9             10        95.0       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"result\"] = encoder.fit_transform(df[\"result\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbc4ff-43c4-44e1-ac44-2bcb41361a41",
   "metadata": {},
   "source": [
    "## Step 6: Feature Scaling\n",
    "\n",
    "Feature scaling ensures that numerical features have similar ranges.This is important because many machine learning algorithms are sensitive\n",
    "to the scale of input data.\n",
    "We use Standardization, which transforms features so that:\n",
    "- Mean ≈ 0\n",
    "- Standard deviation ≈ 1\n",
    "\n",
    "Only input features are scaled the target variable is not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43fe9559-b40d-40df-b922-b04e69531d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.56524758, -1.65278774],\n",
       "       [-1.22983739, -1.21007674],\n",
       "       [-0.89442719, -0.76736574],\n",
       "       [-0.2236068 , -0.32465473],\n",
       "       [ 0.1118034 ,  0.11805627],\n",
       "       [ 0.4472136 ,  0.56076727],\n",
       "       [ 0.78262379,  0.82639387],\n",
       "       [ 1.11803399,  1.00347827],\n",
       "       [ 1.45344419,  1.44618927]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(\"result\", axis=1)\n",
    "y = df[\"result\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6100131-3dcd-47c7-88d1-dbbaba91d76f",
   "metadata": {},
   "source": [
    "### Why Feature Scaling is Important\n",
    "\n",
    "Feature scaling is an essential preprocessing step for many machine learning algorithms. In real-world datasets, different features often exist on very different scales. For example, one feature might represent a small numerical range, while another might have much larger values.\n",
    "\n",
    "If such features are used directly, algorithms that rely on distance calculations or gradient-based optimization may give more importance to features with larger numerical values. This can lead to biased learning and poor model performance.\n",
    "\n",
    "Standardization, which was used in the previous step, transforms features so that they have a mean of zero and a standard deviation of one. This ensures that all features contribute more equally during training.\n",
    "\n",
    "It is also important to note that not all models require feature scaling. Tree-based models such as decision trees and random forests are generally unaffected by feature scale. However, models like logistic regression, support vector machines, and neural networks benefit significantly from scaled data.\n",
    "\n",
    "Understanding why feature scaling is applied helps in choosing the right preprocessing steps for different types of machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96566c4a-615b-4408-8ad3-ba8e5f527f97",
   "metadata": {},
   "source": [
    "### Min–Max Normalization and Z-score Standardization\n",
    "\n",
    "Feature scaling can be performed using different techniques depending on the nature of the data and the machine learning algorithm being used. Two of the most commonly used scaling methods are **Min–Max Normalization** and **Z-score Standardization**.\n",
    "\n",
    "\n",
    "\n",
    "#### Min–Max Normalization\n",
    "\n",
    "Min–Max Normalization rescales the values of a feature to a fixed range, usually between **0 and 1**. This is done by subtracting the minimum value of the feature and dividing by the range (maximum minus minimum).\n",
    "\n",
    "After Min–Max Normalization:\n",
    "- The smallest value becomes 0\n",
    "- The largest value becomes 1\n",
    "- All other values fall between 0 and 1\n",
    "\n",
    "This method is useful when the data does not contain extreme outliers and when the model benefits from bounded input values. It is commonly used in algorithms such as neural networks and distance-based models where relative distances between values matter.\n",
    "\n",
    "However, Min–Max Normalization is sensitive to outliers. A single extreme value can significantly affect the scaling of the entire feature.\n",
    "\n",
    "\n",
    "#### Z-score Standardization\n",
    "\n",
    "Z-score Standardization, also known as standardization, transforms the data so that it has:\n",
    "- A mean of 0\n",
    "- A standard deviation of 1\n",
    "\n",
    "Instead of limiting values to a specific range, this method centers the data around zero and scales it based on how much each value deviates from the mean.\n",
    "\n",
    "Z-score Standardization is widely used when the data follows a roughly normal distribution and is especially effective for algorithms that rely on gradient descent or assume standardized input, such as logistic regression, support vector machines, and linear models.\n",
    "\n",
    "Unlike Min–Max Normalization, standardization is less affected by outliers, making it more stable for real-world datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79857810-8147-432d-ae39-b87398902c52",
   "metadata": {},
   "source": [
    "## Step 7: Splitting the Dataset\n",
    "\n",
    "After completing all preprocessing steps, the dataset is split into separate subsets to ensure fair and reliable model evaluation. Splitting the data allows us to test how well a model performs on unseen data, rather than just memorizing the training examples.\n",
    "\n",
    "In this step, the dataset is divided into three parts:\n",
    "\n",
    "- **Training set (70%)**  \n",
    "  This portion of the data is used to train the machine learning model. The model learns patterns, relationships, and trends from this data by adjusting its internal parameters.\n",
    "\n",
    "- **Validation set (20%)**  \n",
    "  The validation set is used during model development to tune hyperparameters and evaluate intermediate performance. It helps in detecting overfitting and guiding decisions such as model selection and parameter adjustment.\n",
    "\n",
    "- **Test set (10%)**  \n",
    "  The test set is kept completely separate and is used only once the model training and tuning are complete. It provides an unbiased estimate of the model’s final performance on unseen data.\n",
    "\n",
    "This separation is important because it prevents information from the test data from influencing the training process. By evaluating the model on data it has never seen before, we obtain a more realistic measure of how the model will perform in real-world scenarios.\n",
    "\n",
    "Splitting the dataset into training, validation, and test sets is a standard best practice in machine learning and plays a crucial role in avoiding overfitting and ensuring reliable results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cce8e35-372b-49f1-9919-8997373a3936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6\n",
      "Validation samples: 2\n",
      "Test samples: 1\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=1/3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Validation samples:\", len(X_val))\n",
    "print(\"Test samples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b4f56-4dd5-4b1d-a9c9-f80026c2400e",
   "metadata": {},
   "source": [
    "## Data Leakage\n",
    "\n",
    "After splitting the dataset into training, validation, and test sets, it is important to understand the concept of **data leakage**. Data leakage occurs when information from outside the training set is accidentally used during model training, leading to overly optimistic performance results.\n",
    "\n",
    "One common source of data leakage is improper preprocessing order. For example, if scaling or encoding is applied to the entire dataset before splitting, the model indirectly gains information from the validation or test data. This can cause the model to perform well during evaluation but fail in real-world scenarios.\n",
    "\n",
    "To avoid data leakage, preprocessing steps such as scaling and encoding should be fitted only on the training data and then applied to validation and test data using the same parameters.\n",
    "\n",
    "Following a consistent and well-ordered preprocessing pipeline helps ensure that model evaluation is fair and realistic. Awareness of data leakage is an important part of developing reliable machine learning systems and is a key best practice in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339c59e-9be2-4821-9ea3-b6c083f284ad",
   "metadata": {},
   "source": [
    "### Task for the Reader\n",
    "\n",
    "To reinforce your understanding of data handling and preprocessing, complete the following tasks using the steps demonstrated in this module.\n",
    "\n",
    "1. Create a small dataset of your own with at least three features and one target variable.Example ideas include student performance, house prices, or product sales.\n",
    "\n",
    "2. Load the dataset into a pandas DataFrame and inspect its structure using appropriate functions.\n",
    "\n",
    "3. Introduce at least one missing value intentionally and:\n",
    "   - Detect the missing value\n",
    "   - Handle it using either removal or imputation\n",
    "\n",
    "4. Identify any categorical feature in your dataset and convert it into numerical form using an appropriate encoding technique.\n",
    "\n",
    "5. Apply feature scaling using either Min–Max Normalization or Z-score Standardization and observe how the feature values change.\n",
    "\n",
    "6. Split the dataset into training, validation, and test sets using a suitable ratio.\n",
    "\n",
    "7. Briefly explain why the order of preprocessing steps is important and how incorrect ordering can lead to data leakage.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
