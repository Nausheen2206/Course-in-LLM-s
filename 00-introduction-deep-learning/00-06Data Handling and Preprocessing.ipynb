{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e520e76d-2d82-4fcb-a82d-2c3ec6f3a567",
   "metadata": {},
   "source": [
    "# Data Handling and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d5eda1-a645-4a47-8da2-5994630783a2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dataset handling and preprocessing are critical steps in Machine Learning and Deep Learning.\n",
    "The quality of data directly affects the performance, accuracy, and reliability of a model.\n",
    "\n",
    "Raw data often contains:\n",
    "- Missing values\n",
    "- Different scales\n",
    "- Noise and outliers\n",
    "- Bias\n",
    "\n",
    "Preprocessing ensures that the dataset is clean, well-structured, and suitable for training\n",
    "machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe1d34-4f59-469a-b04a-3ac9dd7dc2d6",
   "metadata": {},
   "source": [
    "##  Purpose of Data Splitting\n",
    "\n",
    "Data splitting is the process of dividing a dataset into separate subsets for training,\n",
    "validation, and testing.\n",
    "\n",
    "### Why is data splitting important?\n",
    "- To evaluate model performance on unseen data\n",
    "- To prevent overfitting\n",
    "- To tune hyperparameters correctly\n",
    "- To simulate real-world deployment scenarios\n",
    "\n",
    "Without proper splitting, models may memorize data instead of learning patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54694afa-a575-48fb-a831-9a42b242f93e",
   "metadata": {},
   "source": [
    "##  Training, Validation, and Testing Sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ee284-1d02-47f4-b853-14d6653f0b3c",
   "metadata": {},
   "source": [
    "### Training Set\n",
    "- Used to train the model\n",
    "- Model learns parameters such as weights and biases\n",
    "- Typically contains the largest portion of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b6dc7-8915-402c-8702-280de430e7fa",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "- Used for hyperparameter tuning\n",
    "- Helps in model selection\n",
    "- Prevents overfitting\n",
    "- Not used to update model weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc0ade-1a19-4a2f-b357-7a9244ec8cd4",
   "metadata": {},
   "source": [
    "### Testing Set\n",
    "- Used only after model training is complete\n",
    "- Evaluates final model performance\n",
    "- Should never influence training or validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403b01e-98ef-4637-9702-c82d54acc72d",
   "metadata": {},
   "source": [
    "## Common Split Ratios (70/20/10)\n",
    "\n",
    "A commonly used split ratio is:\n",
    "\n",
    "- 70% Training\n",
    "- 20% Validation\n",
    "- 10% Testing\n",
    "\n",
    "Other popular ratios include:\n",
    "- 80/20\n",
    "- 60/20/20\n",
    "\n",
    "The choice depends on:\n",
    "- Dataset size\n",
    "- Model complexity\n",
    "- Problem domain\n",
    "\n",
    "\n",
    "The below code splits a dataset into training, validation, and testing sets.First, it separates 70% of the data for training and 30% for temporary use.Then, the temporary data is further split into validation and testing sets.Finally, it prints the sizes of each set to confirm the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3b420f-e351-41dd-8466-7406be970b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (35, 2)\n",
      "Validation set size: (10, 2)\n",
      "Testing set size: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.arange(100).reshape(50, 2)\n",
    "y = np.arange(50)\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Validation-Test split\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Validation set size:\", X_val.shape)\n",
    "print(\"Testing set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167eb80c-8eef-4fce-bd83-8eecc5a49f7c",
   "metadata": {},
   "source": [
    "##  Random Sampling vs Stratified Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050b513e-9f41-4f4e-b8d9-cf7e8190c7f7",
   "metadata": {},
   "source": [
    "### Random Sampling\n",
    "\n",
    "Random sampling is a data splitting technique in which samples are selected\n",
    "purely at random from the dataset, giving each data point an equal probability\n",
    "of being assigned to the training or testing set.\n",
    "\n",
    "In random sampling, the selection process does not consider the class labels\n",
    "or target variable distribution. As a result, the proportion of classes in the\n",
    "training and testing sets may differ from that of the original dataset.\n",
    "\n",
    "Random sampling is simple to implement and works well when the dataset is large\n",
    "and balanced. However, for imbalanced datasets, it may lead to biased model\n",
    "evaluation due to unequal class representation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The below code performs a random train–test split on the dataset. It randomly selects 80% of the data for training and 20% for testing, without preserving any class distribution (no stratification).The random_state ensures reproducibility, and the printed shapes confirm the sizes of the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f93331e-f0c8-4816-bacc-5afd984a6244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (40, 2)\n",
      "Testing set shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset\n",
    "X = np.arange(100).reshape(50, 2)   # Features\n",
    "y = np.arange(50)                  # Target labels\n",
    "\n",
    "# Random sampling (no stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c99586-0113-4ba1-83a3-a7cd4839d953",
   "metadata": {},
   "source": [
    "### Stratification \n",
    "\n",
    "Stratification is a data sampling technique in which the dataset is divided\n",
    "into homogeneous subgroups called *strata* based on the target variable.\n",
    "Samples are then drawn from each stratum in such a way that the proportion\n",
    "of each class is preserved in the training and testing sets.\n",
    "\n",
    "Stratified sampling is mainly used in **classification problems**, especially\n",
    "when the dataset is **imbalanced**. It ensures fair representation of all\n",
    "classes during model training and evaluation.\n",
    "\n",
    "Unlike random sampling, stratification prevents situations where minority\n",
    "classes are underrepresented or completely missing in the training or\n",
    "testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cafdea1-f508-442b-9e63-c28e0c7a8d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (40, 2)\n",
      "Testing set shape: (10, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset with classes\n",
    "X = np.arange(100).reshape(50, 2)\n",
    "y = np.array([0]*25 + [1]*25)  # Balanced classes\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7a8f24-5e65-41aa-a02d-35d5d6f4f428",
   "metadata": {},
   "source": [
    "### What is Data Leakage?\n",
    "\n",
    "Data leakage is a serious problem in machine learning that occurs when\n",
    "information from outside the training dataset is unintentionally used\n",
    "during model training.\n",
    "\n",
    "This leaked information gives the model access to data it would not have\n",
    "in real-world scenarios, leading to unrealistically high performance\n",
    "during training or evaluation.\n",
    "\n",
    "As a result, the model performs poorly when deployed on new, unseen data.\n",
    "\n",
    "#### How Data Leakage Happens\n",
    "\n",
    "Data leakage can occur due to:\n",
    "- Applying preprocessing (scaling, normalization) before data splitting\n",
    "- Using test data during model training\n",
    "- Creating features using future or target-related information\n",
    "- Performing feature selection on the entire dataset\n",
    "\n",
    "Even small leaks can severely distort model evaluation results.\n",
    "\n",
    "\n",
    "#### Effects of Data Leakage\n",
    "- Overestimated model accuracy\n",
    "- Poor generalization to unseen data\n",
    "- Incorrect model selection\n",
    "- Failure in real-world deployment\n",
    "\n",
    "#### How to Avoid Data Leakage\n",
    "- Always split the data before preprocessing\n",
    "- Fit preprocessing steps only on training data\n",
    "- Never use test data during training or validation\n",
    "- Apply the same preprocessing parameters to validation and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739373fd-a3f9-4ba1-8631-9253dc781153",
   "metadata": {},
   "source": [
    "## Importance of Feature Scaling\n",
    "\n",
    "Feature scaling is a preprocessing technique used to bring all input features\n",
    "to a similar scale so that no single feature dominates the learning process\n",
    "due to its magnitude.\n",
    "\n",
    "In real-world datasets, different features often have different units and\n",
    "ranges. For example:\n",
    "- Age may range from 0 to 100\n",
    "- Salary may range from 1,000 to 1,000,000\n",
    "- Distance may be measured in kilometers\n",
    "\n",
    "Without scaling, machine learning models may give unfair importance to\n",
    "features with larger numerical values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fa44b-7cbc-4a87-a1f3-a82673c2df65",
   "metadata": {},
   "source": [
    "### Example of Feature Scale\n",
    "\n",
    "Consider the following features in a dataset:\n",
    "\n",
    "- Age: ranges from 18 to 60\n",
    "- Height (cm): ranges from 150 to 190\n",
    "- Salary (INR): ranges from 15,000 to 1,000,000\n",
    "\n",
    "Although all three are important, Salary has a much larger numerical scale\n",
    "than Age or Height.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ffd46-49a4-49b6-b05d-6b354948c1c3",
   "metadata": {},
   "source": [
    "### Why Scale Matters\n",
    "\n",
    "Many machine learning algorithms use:\n",
    "- Distance calculations\n",
    "- Gradient optimization\n",
    "\n",
    "If features are on different scales, the algorithm will give more importance\n",
    "to features with larger values, even if they are not more important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c172dcd-594d-4141-b74d-e5a2439939d8",
   "metadata": {},
   "source": [
    "### Meaning of Scaling a Feature\n",
    "\n",
    "Scaling a feature means transforming its values so that:\n",
    "- All features have comparable ranges\n",
    "- No feature dominates due to its magnitude\n",
    "- The learning algorithm treats features fairly\n",
    "\n",
    "Scaling does NOT change the meaning of the data; it only changes how values\n",
    "are represented numerically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd09cf44-b037-48a8-bc52-ac7e58629d44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Imagine measuring:\n",
    "- Distance in kilometers\n",
    "- Weight in grams\n",
    "\n",
    "If used together without adjustment, grams will dominate numerically.\n",
    "Scaling converts both measurements into a comparable form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7b21f7-29a9-4834-b2ad-ae7030ded67f",
   "metadata": {},
   "source": [
    "## Min–Max Normalization\n",
    "\n",
    "Min–Max Normalization is a feature scaling technique that transforms data\n",
    "to a fixed range, usually between 0 and 1.\n",
    "\n",
    "It rescales the feature values so that the minimum value becomes 0\n",
    "and the maximum value becomes 1.\n",
    "\n",
    "This technique preserves the original distribution shape of the data\n",
    "but changes the scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87078a-5c05-4834-b3b0-fccf024efce7",
   "metadata": {},
   "source": [
    "### When to Use Min–Max Normalization\n",
    "\n",
    "- When the data has a known fixed range\n",
    "- When the algorithm requires bounded input values\n",
    "- Commonly used in Neural Networks and Image Processing\n",
    " Sensitive to outliers, because extreme values affect min and max.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de12a672-5503-457b-ac63-99f03bf6d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  ],\n",
       "       [0.25],\n",
       "       [0.5 ],\n",
       "       [0.75],\n",
       "       [1.  ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[10], [20], [30], [40], [50]])\n",
    "\n",
    "# Apply Min-Max Normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_minmax = scaler.fit_transform(X)\n",
    "\n",
    "X_minmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fa52a-0619-4b7c-9aaa-1e237b0892df",
   "metadata": {},
   "source": [
    "## Z-score Standardization\n",
    "\n",
    "Z-score Standardization (also called Standard Scaling) transforms data\n",
    "so that it has:\n",
    "- Mean = 0\n",
    "- Standard Deviation = 1\n",
    "\n",
    "This technique measures how many standard deviations a value is\n",
    "away from the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30717e2d-df72-4f2b-b12c-ff5ac512fcb8",
   "metadata": {},
   "source": [
    "### Formula for Z-score Standardization\n",
    "\n",
    "z = (x − μ) / σ\n",
    "\n",
    "Where:\n",
    "- μ (mu) is the mean of the feature\n",
    "- σ (sigma) is the standard deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b2953-5cd8-42fc-87bf-c74f6319b7e7",
   "metadata": {},
   "source": [
    "### When to Use Z-score Standardization\n",
    "\n",
    "- When data follows a normal (Gaussian) distribution\n",
    "- When outliers exist\n",
    "- Required for algorithms like:\n",
    "  - Linear Regression\n",
    "  - Logistic Regression\n",
    "  - SVM\n",
    "  - PCA\n",
    "  - Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21238d6-ace4-4223-a30d-69a4dfd700e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.41421356],\n",
       "       [-0.70710678],\n",
       "       [ 0.        ],\n",
       "       [ 0.70710678],\n",
       "       [ 1.41421356]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[10], [20], [30], [40], [50]])\n",
    "\n",
    "# Apply Z-score Standardization\n",
    "scaler = StandardScaler()\n",
    "X_zscore = scaler.fit_transform(X)\n",
    "\n",
    "X_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f68189-2397-4be1-b768-6ca2f09a016a",
   "metadata": {},
   "source": [
    "The code applies Z-score standardization to the dataset.\n",
    "It rescales the values so that the transformed data has a mean of 0 and a standard deviation of 1, making the feature suitable for machine learning algorithms that are sensitive to feature scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8fb6d-7df3-48b1-9ecf-41de953e8bc2",
   "metadata": {},
   "source": [
    "## Mean and Standard Deviation Normalization (Image Data)\n",
    "\n",
    "In image preprocessing, pixel values are normalized using the dataset's\n",
    "mean and standard deviation.\n",
    "\n",
    "Images usually have pixel values in the range [0, 255].\n",
    "Normalization helps neural networks train faster and more stably.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2da95d6f-7265-4c83-bf2a-d96a9dd7ac35",
   "metadata": {},
   "source": [
    "### Why Image Normalization is Important\n",
    "\n",
    "- Improves convergence speed\n",
    "- Prevents exploding or vanishing gradients\n",
    "- Ensures stable training in deep neural networks\n",
    "- Standard practice in CNNs\n",
    "\n",
    "\n",
    "## Formula for Image Normalization\n",
    "\n",
    "x_normalized = (x − mean) / standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77377280-3c33-40cf-8d76-4d3d0bdf7b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simulated RGB image (height=224, width=224, channels=3)\n",
    "image = np.random.randint(0, 256, (224, 224, 3))\n",
    "\n",
    "# Compute mean and std\n",
    "mean = image.mean()\n",
    "std = image.std()\n",
    "\n",
    "# Normalize image\n",
    "normalized_image = (image - mean) / std\n",
    "\n",
    "normalized_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c7650-d9d1-449c-8695-abc8cbbdf5ba",
   "metadata": {},
   "source": [
    "This code performs mean and standard deviation normalization on an RGB image.\n",
    "It computes the image’s mean and standard deviation, then normalizes all pixel values so the image has approximately zero mean and unit variance, which helps improve stability and training efficiency in deep learning models.\n",
    "\n",
    "\n",
    "\n",
    "In deep learning frameworks (PyTorch / TensorFlow), predefined mean and\n",
    "standard deviation values are often used, especially for pretrained models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c6667b-e424-481d-847a-c263288d9498",
   "metadata": {},
   "source": [
    "## Consistent Preprocessing for Training and Testing Data\n",
    "\n",
    "Consistent preprocessing means applying the same transformations\n",
    "to training, validation, and testing datasets using identical parameters.\n",
    "\n",
    "This is critical to ensure fair and unbiased model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ea925-dbcc-47a5-88ed-0d4c64d81835",
   "metadata": {},
   "source": [
    "### Correct Rule to Follow\n",
    "\n",
    "- Fit preprocessing steps ONLY on training data\n",
    "- Apply (transform) the same preprocessing to validation and test data\n",
    "\n",
    "Never fit preprocessing on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721a730-eaae-4dd5-a708-1291b86a85ef",
   "metadata": {},
   "source": [
    "### Why Consistency is Important\n",
    "\n",
    "If preprocessing parameters differ:\n",
    "- Data leakage occurs\n",
    "- Model evaluation becomes invalid\n",
    "- Test accuracy becomes misleading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71226976-eb5e-4b6c-81dc-e6376d19c858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.17824708,  0.23250366],\n",
       "        [-0.91949478, -1.04884983],\n",
       "        [-1.17824708, -1.50352364]]),\n",
       " array([[ 1.26141747,  1.51385714],\n",
       "        [-1.03038863,  1.72052706],\n",
       "        [ 1.11355901, -0.55284203]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.random.randint(10, 100, (100, 2))\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit scaler ONLY on training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply same scaler to test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled[:3], X_test_scaled[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1d224-029b-471d-9cf7-bd2c00a96c38",
   "metadata": {},
   "source": [
    "This code demonstrates consistent preprocessing to avoid data leakage.\n",
    "The dataset is first split into training and testing sets.\n",
    "The StandardScaler is fitted only on the training data and then applied to both training and test sets using the same parameters, ensuring fair and unbiased model evaluation.\n",
    "\n",
    "\n",
    "### Incorrect Approach (Leads to Data Leakage)\n",
    "\n",
    "Fitting the scaler separately on training and test data\n",
    "introduces test data information into preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b6a3a-3f28-4ac8-83b9-c2d0b2b02a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
