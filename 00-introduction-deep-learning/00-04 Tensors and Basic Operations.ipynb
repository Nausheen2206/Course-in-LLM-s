{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea984eda-a0f0-4755-852a-26a8607f9fdf",
   "metadata": {},
   "source": [
    "# Tensors and Basic Operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02791d-9e1b-4f49-995b-7b1bae62b9f0",
   "metadata": {},
   "source": [
    "## Installing TensorFlow\n",
    "To install TensorFlow on your local machine you can use pip.\n",
    "```console\n",
    "pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e9393-b76a-4bb2-b791-e45a864ded3e",
   "metadata": {},
   "source": [
    "## Tensors \n",
    "A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe32489-0f65-4512-aa1c-b9e075e2de47",
   "metadata": {},
   "source": [
    "**Data Types Include**: float32, int32, string and others.\n",
    "\n",
    "**Shape**: Represents the dimension of data.\n",
    "\n",
    "Just like vectors and matrices tensors can have operations applied to them like addition, subtraction, dot product, cross product etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52c0ef-0f5c-4c0a-9bab-d8942246dcc8",
   "metadata": {},
   "source": [
    "### Scalar\n",
    "- A tensor with **zero dimensions**\n",
    "- Represents a single numerical value\n",
    "\n",
    "Example:  \n",
    "x = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a536c849-bd22-4593-a3f7-b9e118ba31ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "scalar = torch.tensor(5)\n",
    "scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242a397-1a8f-4280-be10-759cf16e9408",
   "metadata": {},
   "source": [
    "### Vector\n",
    "- A tensor with **one dimension**\n",
    "- Represents a list of numbers\n",
    "\n",
    "Example:  \n",
    "x = [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dce20c-4e09-4efb-b1b4-753323dfae4d",
   "metadata": {},
   "source": [
    "### Matrix\n",
    "- A tensor with **two dimensions**\n",
    "- Represents rows and columns\n",
    "\n",
    "Example:  \n",
    "x = [[1, 2],  \n",
    "     [3, 4]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61f49dc-c230-49c4-8976-75ae82bf8cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565efc27-7d84-41db-b979-041158ed115a",
   "metadata": {},
   "source": [
    "### Higher-Order Tensors\n",
    "- Tensors with **three or more dimensions**\n",
    "- Used to represent images, videos, and batches of data\n",
    "\n",
    "Example:\n",
    "- 3D tensor → RGB image\n",
    "- 4D tensor → batch of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c05a923-dc4a-4421-b2dc-7344fcf63305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_3d = torch.randn(2, 3, 4)\n",
    "tensor_3d.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d46b05-edd5-4f46-8c82-4584f2af1b44",
   "metadata": {},
   "source": [
    "### 1. Creating a Tensor from Python Data\n",
    "\n",
    "Tensors can be created directly from Python lists or nested lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "238b2484-7ce4-4211-9784-abfea34b9da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10),\n",
       " tensor([1, 2, 3, 4]),\n",
       " tensor([[1, 2, 3],\n",
       "         [4, 5, 6]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Scalar\n",
    "scalar = torch.tensor(10)\n",
    "\n",
    "# Vector\n",
    "vector = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "# Matrix\n",
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "scalar, vector, matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1243f4-7d37-4950-b49a-a730bf7c2d8b",
   "metadata": {},
   "source": [
    "### 2. Creating Tensors with Specific Values\n",
    "\n",
    "Deep learning frameworks provide functions to create tensors filled with predefined values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b334ec-7c0f-4ae8-8029-743e5c3fd600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor filled with zeros\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "\n",
    "# Tensor filled with ones\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "\n",
    "# Identity matrix\n",
    "identity_tensor = torch.eye(3)\n",
    "\n",
    "zeros_tensor, ones_tensor, identity_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9d6f5-2253-47d1-b56f-5d0e1853075c",
   "metadata": {},
   "source": [
    "### 3. Creating Random Tensors\n",
    "\n",
    "Random tensors are widely used to initialize weights in neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a130067-fbe7-4bfe-a478-39f392a7d2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9042, 0.0924, 0.8552],\n",
       "         [0.7995, 0.1496, 0.0989]]),\n",
       " tensor([[-0.6224, -0.5790,  1.2360],\n",
       "         [ 1.8613,  0.0829, -1.0737]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random values between 0 and 1\n",
    "random_uniform = torch.rand(2, 3)\n",
    "\n",
    "# Random values from normal distribution (mean=0, std=1)\n",
    "random_normal = torch.randn(2, 3)\n",
    "\n",
    "random_uniform, random_normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da04ba0-34bf-4c91-82ea-b2e119bb59f1",
   "metadata": {},
   "source": [
    "### 4. Creating Tensors with a Specific Data Type\n",
    "\n",
    "Tensors can be created with specific data types such as integers or floating-point values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abeb5807-9bd0-4a05-a038-4ef8dd0e9448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_tensor = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "\n",
    "float_tensor.dtype, int_tensor.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e466e-3d76-4df8-8922-92c348d0d322",
   "metadata": {},
   "source": [
    "### 5. Creating Tensors with a Specific Shape\n",
    "\n",
    "Sometimes we need tensors of a particular shape for neural network layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df1efb5-6463-4ab1-b3f3-d4564eadc9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty tensor (values are uninitialized)\n",
    "empty_tensor = torch.empty(3, 2)\n",
    "\n",
    "# Create a tensor with the same shape as another tensor\n",
    "reference = torch.ones(2, 2)\n",
    "same_shape = torch.zeros_like(reference)\n",
    "\n",
    "empty_tensor, same_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2c76c-b12f-4d8a-a584-ce37e771627f",
   "metadata": {},
   "source": [
    "### 6. Creating Tensors Using Ranges\n",
    "\n",
    "Range-based tensors are useful for indexing and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f6ee42-3a7c-405b-82a1-4948a04aec1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence of numbers\n",
    "range_tensor = torch.arange(0, 10)\n",
    "\n",
    "# Evenly spaced values\n",
    "linspace_tensor = torch.linspace(0, 1, steps=5)\n",
    "\n",
    "range_tensor, linspace_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c67fa-27fe-4860-9d19-82d452e69c0c",
   "metadata": {},
   "source": [
    "### 7. Creating Tensors That Track Gradients\n",
    "\n",
    "For training neural networks, tensors must track gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63db2bec-d35b-44bf-8c8a-789770138c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff3724-bb61-4fe6-8178-6fb9b344b8a3",
   "metadata": {},
   "source": [
    "## Indexing and Slicing of Tensors\n",
    "\n",
    "Indexing and slicing are used to access or extract specific elements or parts of a tensor.  \n",
    "This is very important in deep learning for selecting features, batches, channels, and regions of data.\n",
    "\n",
    "### What is Indexing?\n",
    "\n",
    "Indexing means selecting **individual elements** of a tensor using their position (index).  \n",
    "Indexing in tensors starts from **0**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d11c8028-4dba-45cc-858f-8ce275c1925c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 1D tensor (vector)\n",
    "x = torch.tensor([10, 20, 30, 40, 50])\n",
    "x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f38dd-8ae7-4515-9739-34be1c37e20d",
   "metadata": {},
   "source": [
    "### 1. Indexing a 1D Tensor (Vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "418f4fbf-46d7-4fe6-8f58-361200826b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]   # first element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17917afa-2fa4-4330-b22d-201a35d3a318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2]   # third element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c03eecf8-d246-4679-916e-9d42b694d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1]  # last element\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b42a1-6234-4a39-af5d-e5cb6adc8d57",
   "metadata": {},
   "source": [
    "- Positive index → counts from the beginning  \n",
    "- Negative index → counts from the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadb232-a5b6-4a8f-b861-8879631f808c",
   "metadata": {},
   "source": [
    "### 2. Slicing a 1D Tensor\n",
    "\n",
    "Slicing extracts a **range of elements** from a tensor.\n",
    "Syntax: `start : end` (end index is excluded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7896f02d-2104-474c-9dd5-1cfa008f1b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 30, 40])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:4]   # elements from index 1 to 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a493d184-5dab-4115-8dc9-620ed95b5c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:3]    # first three elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "958e1968-6f1c-4b7f-a5fc-5161b92363fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 40, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2:]    # elements from index 2 to end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d9528-247f-4142-8b3c-94e4cd353559",
   "metadata": {},
   "source": [
    "### Step Size in Slicing\n",
    "\n",
    "Syntax: `start : end : step`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe2086fa-e600-4c74-880a-c8c75441cc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 30, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[::2]   # every second element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed8db50-5a37-46f4-8b56-b203d22b1881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 40, 30, 20, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([10, 20, 30, 40, 50])\n",
    "torch.flip(x, dims=[0])\n",
    " # reverse the tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac5591-0f7f-4b2c-a827-5a2642333a3a",
   "metadata": {},
   "source": [
    "### 3. Indexing a 2D Tensor (Matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "754e6c14-3c9a-4168-91fa-ab4193417f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 2D tensor\n",
    "m = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71770c-9da6-4feb-8e3e-f1f9aa828e72",
   "metadata": {},
   "source": [
    "### Accessing Rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f4fd260-35a6-4c03-bafa-c3aa7307575d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0]     # first row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb64018-db30-4e37-89b6-e2095acba837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[1]     # second row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edba33b8-335e-4b92-ad21-da17926b6845",
   "metadata": {},
   "source": [
    "### Accessing Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c143ec-c2d8-4afe-96b6-34137d320112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 0]  # first column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b6de3b-8d36-4b13-864d-8244434497a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 2]  # third column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be523d3-54c4-4c0c-b922-6ce51d162cf4",
   "metadata": {},
   "source": [
    "### 3. Slicing a 2D Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67f7d87-ee8e-414c-bf2d-f9b83c12dda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0:2, 1:3]  # rows 0–1 and columns 1–2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "391acc65-edcb-4a3d-922a-d26a3c308e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 1:]     # all rows, columns from index 1 onward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae6320f5-feb7-469a-82e2-a802c6df6472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[:, 1:]     # all rows, columns from index 1 onward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcdabe-d541-4fcc-bd65-efb7b079e06f",
   "metadata": {},
   "source": [
    "### 4. Indexing Higher-Dimensional Tensors\n",
    "\n",
    "Higher-order tensors are common in deep learning.\n",
    "Example: 3D tensor (batch, rows, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27f55752-f247-4a34-8e01-912c1232752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(2, 3, 4)\n",
    "t.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf5640-b42d-4bdd-9b26-f691d5a85171",
   "metadata": {},
   "source": [
    "### 5. Using Ellipsis (...)\n",
    "\n",
    "Ellipsis is used to represent missing dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b2591e9-900b-465a-a740-2bf500844bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7403, -1.7540,  1.4743],\n",
       "        [ 1.6449,  0.4153,  1.2084]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1]   # all elements from last dimension index 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2d248-ac6f-4276-b7f4-80d987bb65f6",
   "metadata": {},
   "source": [
    "### 6. Boolean Indexing\n",
    "\n",
    "Boolean indexing selects elements based on conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74dc9184-7898-4809-85d1-ffce0795e547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 20])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5, 10, 15, 20])\n",
    "x[x > 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ffe23-2198-4d71-ba0d-f465e8ae92b2",
   "metadata": {},
   "source": [
    "## Reshaping Tensors\n",
    "\n",
    "Reshaping means changing the **shape (dimensions)** of a tensor **without changing its data**.  \n",
    "In deep learning, reshaping is very common when preparing data for neural network layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7482b-07fa-4343-b53e-84ea48649265",
   "metadata": {},
   "source": [
    "### Why Reshaping is Needed in Deep Learning\n",
    "\n",
    "- Neural network layers expect inputs in specific shapes  \n",
    "- Images, text, and batches must be reshaped correctly  \n",
    "- Helps convert data between layers (e.g., CNN → Fully Connected layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828bcf91-1bef-4252-b871-9be3b3a56e8c",
   "metadata": {},
   "source": [
    "### Understanding Tensor Shape\n",
    "\n",
    "The **shape** of a tensor tells:\n",
    "- How many dimensions it has  \n",
    "- How many elements are in each dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "539232cc-5157-440d-9be3-1ef40857c640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0801c2-c771-45e2-a3ec-28d38bfa5245",
   "metadata": {},
   "source": [
    "### 1. Reshaping Using `reshape()`\n",
    "\n",
    "The `reshape()` function changes the shape of a tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd1ca4fc-d36a-4645-99e3-133e691236ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "x_reshaped = x.reshape(2, 3)\n",
    "x_reshaped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19e20c-5066-4952-b027-b44a46f2bf04",
   "metadata": {},
   "source": [
    "Rule:  \n",
    "Total number of elements **must remain the same**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a89d87-2667-42c3-9bd0-d6546f8ff839",
   "metadata": {},
   "source": [
    "### 2. Using `-1` to Automatically Infer Size\n",
    "\n",
    "PyTorch can automatically calculate one dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8eb4209-1310-4a70-8793-ba3c841118da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d95243-f0a1-4bfd-8c4f-5d0d439855f8",
   "metadata": {},
   "source": [
    "Here, PyTorch finds the correct value for `-1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d41e8-4f78-44b2-8071-9875d390871b",
   "metadata": {},
   "source": [
    "### 3. Flattening a Tensor\n",
    "\n",
    "Flattening means converting a multi-dimensional tensor into a 1D tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5791da29-325a-428a-9b92-7bfcb8db00fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "m.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7b5da-232b-4cbc-8b98-59eb67f84646",
   "metadata": {},
   "source": [
    "### 4. `view()` vs `reshape()`\n",
    "\n",
    "Both change the shape of a tensor, but they differ internally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e324dbc-84a0-44f6-bce6-ebca37a38833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "x.view(2, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f490b-c919-4bd7-831c-23e7eb8a7032",
   "metadata": {},
   "source": [
    "- `view()` works only if tensor memory is contiguous  \n",
    "- `reshape()` is safer and preferred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a087d24-c3b0-4e9f-9a88-260f60017471",
   "metadata": {},
   "source": [
    "### 5. Adding or Removing Dimensions\n",
    "\n",
    "Sometimes we need to add or remove dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "242a56b5-e13a-4595-9698-f426a1cbd464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "\n",
    "x.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70f1f0ec-a2de-4500-8056-020cd629a060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x.unsqueeze(0).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f520e5-325f-41df-bb87-f9c9c1f6b262",
   "metadata": {},
   "source": [
    "## Tensor Arithmetic Operations\n",
    "\n",
    "Tensor arithmetic operations are mathematical operations performed on tensors.  \n",
    "These operations are applied **element-wise** and are heavily used in deep learning for computing outputs, losses, and gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f82b90d6-cb8b-4449-b9ce-9c0cb4e0dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e3e50a1-6af6-4374-89f2-f4f5d1c50c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Addition\n",
    "a + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc9504b7-8a92-436b-a717-ec36e12d8caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3, -3, -3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtraction\n",
    "a - b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66fdc5cc-f9d9-4ca4-967b-928dc01b2a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10, 18])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplication (element-wise)\n",
    "a * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37d24e6b-b195-4e30-b364-f13f2d97354c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2500, 0.4000, 0.5000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Division\n",
    "a / b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0634fc-de18-4def-ab77-895e98c2c79f",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8e1e40e-b989-4466-ad1b-a6fc573d3d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19, 22],\n",
       "        [43, 50]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = torch.tensor([[1, 2],\n",
    "                   [3, 4]])\n",
    "m2 = torch.tensor([[5, 6],\n",
    "                   [7, 8]])\n",
    "\n",
    "torch.matmul(m1, m2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59bd6f-1d21-4020-aa82-9f93b7df67dd",
   "metadata": {},
   "source": [
    "## Broadcasting \n",
    "\n",
    "Broadcasting allows arithmetic operations between tensors of **different shapes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205aa1b0-43f3-4318-a989-72e4714e487b",
   "metadata": {},
   "source": [
    "### Rules of Broadcasting\n",
    "1. Compare tensor shapes from right to left  \n",
    "2. Dimensions are compatible if they are equal or one of them is 1  \n",
    "3. Smaller tensor is automatically expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99f4ed39-0392-4bee-81a7-3a644cd944a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4, 6],\n",
       "        [5, 7, 9]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "y = torch.tensor([1, 2, 3])\n",
    "\n",
    "x + y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a709c24-aab4-491a-9150-9f034216ba41",
   "metadata": {},
   "source": [
    "Here, `y` is broadcast across rows of `x`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959dd19-5f4f-4f02-9adc-75234051a736",
   "metadata": {},
   "source": [
    "Broadcasting avoids copying data and improves performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb600fcd-13ed-4411-be17-85a2b52e5981",
   "metadata": {},
   "source": [
    "The output shows the **probabilities** of each class.  \n",
    "Softmax is ideal for problems with >2 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e935a17-399e-45aa-85ae-83a3d77c3e1d",
   "metadata": {},
   "source": [
    "## Tensors and Automatic differentiation \n",
    "Automatic differentiation (AD) is a fundamental technique in machine learning, particularly in frameworks like TensorFlow. It is crucial for model optimization techniques like gradient descent since it improves the efficiency of function gradient computation. Complex model creation and training are made simpler by AD's easy integration into TensorFlow's computational network, which eliminates the need for manual gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e4cf4-f6e6-4da2-ab43-f12672aa4f7e",
   "metadata": {},
   "source": [
    "## Why do we need differentiation in Deep Learning?\n",
    "\n",
    "When we train a neural network, our goal is very simple:\n",
    "**reduce the error (loss)**.\n",
    "\n",
    "To reduce the error, the model needs to know:\n",
    "> *If I slightly change this weight, will the loss increase or decrease?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cced2e-3226-4e49-ae74-086c8f8d98dc",
   "metadata": {},
   "source": [
    "That information comes from derivatives.\n",
    "\n",
    "So in practice:\n",
    "- Forward pass → compute the loss  \n",
    "- Backward pass → compute derivatives  \n",
    "- Update weights → repeat\n",
    "Without derivatives, the model cannot learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72b029-0a0e-48f7-a893-4207dbc68023",
   "metadata": {},
   "source": [
    "## Key Concepts of Automatic Differentiation in Tensorflow\n",
    "### 1.Computatinal Graph\n",
    "\n",
    "Computational graphs are used to represent mathematical expressions in a structured way. In deep learning, they act like a descriptive language that explains how a model performs its computations. Formally, a computational graph is a directed graph that shows how values are computed and how data flows through different operations.\n",
    "\n",
    "A computational graph supports two kinds of calculations: forward computation and backward computation. During forward computation, input values move through the graph and each operation produces an output, eventually giving the final result. During backward computation, gradients are calculated by moving backward through the graph, which is essential for training neural networks.\n",
    "\n",
    "In a computational graph, variables are represented as nodes. These variables can be scalars, vectors, matrices, tensors, or other data structures. Edges represent data dependencies or function arguments, showing how one variable influences another. Operations such as addition, subtraction, or multiplication are also represented in the graph, and more complex functions are built by combining these simple operations.\n",
    "\n",
    "For example, consider the expression\n",
    "\n",
    "\\[\n",
    "Y = (a + b) * (b - c)\n",
    "\\]\n",
    "\n",
    "To make the computation clearer, we introduce intermediate variables:\n",
    "\n",
    "\\[\n",
    "d = a + b\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "e = b - c\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "Y = d * e\n",
    "\\]\n",
    "\n",
    "Each of these steps becomes a node in the computational graph, and the arrows show how outputs from one operation are used as inputs for the next.\n",
    "\n",
    "\n",
    "Each of these steps becomes a node in the computational graph, and the arrows show how outputs from one operation are used as inputs for the next.\n",
    "\n",
    "In deep learning, computational graphs explain why training is divided into two phases. The forward pass computes the output and loss of the neural network, while the backward pass computes gradients using the chain rule. These gradients tell us how changes in inputs or parameters affect the final output.\n",
    "\n",
    "When computing derivatives, the key idea is understanding how a small change in one variable affects another variable that depends on it. If a variable directly or indirectly influences the output, its contribution is captured using partial derivatives. By applying the chain rule across the graph, backpropagation efficiently computes derivatives with respect to all input variables.\n",
    "\n",
    "There are two main types of computational graphs. \n",
    "\n",
    "**Static computational graphs** are defined completely before execution, which allows strong optimization and faster performance but makes them less flexible for variable-sized or dynamic data.\n",
    "\n",
    "**Dynamic computational graphs** are built during execution, making them easier to debug and more flexible, though they offer fewer opportunities for global optimization.\n",
    "\n",
    "![Computational Graph](https://media.geeksforgeeks.org/wp-content/uploads/20200527151747/e19.png)\n",
    "\n",
    "![Computational Graph1](https://media.geeksforgeeks.org/wp-content/uploads/20200527173723/e34.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88bcf9-0706-4e69-a431-fd9ed14f03dd",
   "metadata": {},
   "source": [
    "### 2.Gradient Boosting\n",
    "\n",
    "Gradient Boosting is a powerful machine learning technique that builds a strong model by combining many weak models. The main idea is simple: instead of trying to build one perfect model, we build models step by step, where each new model focuses on correcting the mistakes made by the previous ones.\n",
    "\n",
    "In Gradient Boosting, models are added sequentially, not independently. Each new model is trained to reduce the error/loss of the overall system. This is why it is called boosting ,every new model boosts the performance of the existing ensemble.\n",
    "\n",
    "The process begins with a simple model, often one that makes very rough predictions. We then calculate how wrong these predictions are using a loss function. The next model is trained to predict these errors. This process is repeated many times, gradually improving the model’s predictions.\n",
    "\n",
    "The term *gradient* comes from optimization. At each step, Gradient Boosting uses the gradient of the loss function to decide how the next model should improve the predictions. In other words, the model moves in the direction that most reduces the error.\n",
    "\n",
    "Decision trees are commonly used as the weak learners in Gradient Boosting. These trees are usually shallow, meaning they are simple and make limited decisions. Even though each tree is weak on its own, combining many of them results in a very strong predictive model.\n",
    "\n",
    "Gradient Boosting works well for both regression and classification problems. It is widely used because it can capture complex patterns in data and often delivers high accuracy. However, it can be sensitive to noise and overfitting if not properly tuned.\n",
    "\n",
    "Important parameters in Gradient Boosting include the number of trees, learning rate, and tree depth. The learning rate controls how much each new model contributes, while the number of trees determines how long the boosting process continues.\n",
    "\n",
    "Because Gradient Boosting builds models sequentially, training can be slower compared to methods like bagging. However, the performance gains often justify the additional computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74487f2c-89f5-406e-9948-bd8a6fc33125",
   "metadata": {},
   "source": [
    "## Task for the Reader  \n",
    "Using a deep learning framework of your choice (TensorFlow or PyTorch), perform the following tasks to understand tensors and their basic operations.\n",
    "\n",
    "a) Create a scalar tensor, a 1-D tensor, and a 2-D tensor. Print each tensor along with its shape and rank.\n",
    "\n",
    "b) Perform element-wise addition and element-wise multiplication on two tensors of the same shape.\n",
    "\n",
    "c) Multiply a tensor by a scalar value and observe how the values change.\n",
    "\n",
    "d) Access a specific element from a tensor using indexing and extract a sub-part of a tensor using slicing.\n",
    "\n",
    "e) Reshape a tensor into a different shape and explain how reshaping affects the tensor without changing the total number of elements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
