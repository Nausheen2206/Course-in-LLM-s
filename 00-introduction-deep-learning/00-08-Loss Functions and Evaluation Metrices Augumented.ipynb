{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f12ce63",
   "metadata": {},
   "source": [
    "\n",
    "# Loss Functions and Evaluation Metrics\n",
    "\n",
    "Loss functions, optimization techniques, and evaluation metrics form the\n",
    "mathematical and conceptual foundation of training neural networks.\n",
    "This chapter provides a detailed and rigorous explanation of these concepts,\n",
    "focusing on intuition, mathematical meaning, and practical relevance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a666c",
   "metadata": {},
   "source": [
    "\n",
    "## Concept of Loss Functions\n",
    "\n",
    "A loss function is a mathematical function that quantifies how well a model’s\n",
    "predictions match the true target values. It transforms the difference between\n",
    "prediction and ground truth into a single numerical value.\n",
    "\n",
    "In supervised learning, every prediction made by the model produces an error.\n",
    "The loss function aggregates these errors and provides a measurable signal\n",
    "that indicates how incorrect the model is. This signal is essential because\n",
    "learning algorithms require a numeric objective to optimize.\n",
    "\n",
    "Loss functions serve as the **bridge between prediction and learning**.\n",
    "Without a loss function, the model would have no direction for improvement.\n",
    "During training, model parameters are adjusted to minimize the loss value,\n",
    "thereby improving prediction quality over time.\n",
    "\n",
    "Different tasks require different loss functions. For example:\n",
    "- Regression tasks use losses that measure numerical distance.\n",
    "- Classification tasks use losses that compare probabilities.\n",
    "- Ranking and structured tasks require specialized losses.\n",
    "\n",
    "Choosing an appropriate loss function is critical, as it directly influences\n",
    "how the model learns and what types of errors it prioritizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf63d86",
   "metadata": {},
   "source": [
    "\n",
    "The plot shows how **MSE penalizes large errors more aggressively**\n",
    "than MAE. This explains why MSE is sensitive to outliers while\n",
    "MAE is more robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d84d2",
   "metadata": {},
   "source": [
    "\n",
    "## Regression Losses\n",
    "\n",
    "Regression problems involve predicting continuous numerical values.\n",
    "Regression loss functions measure how far predicted values deviate\n",
    "from actual target values.\n",
    "\n",
    "These losses focus on **magnitude of error**, rather than correctness\n",
    "of class assignment. The most commonly used regression losses are\n",
    "Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7b081",
   "metadata": {},
   "source": [
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "Mean Squared Error computes the average of the squared differences\n",
    "between predicted values and actual values.\n",
    "\n",
    "MSE = (1/n) Σ (y − ŷ)²\n",
    "\n",
    "Squaring the error has two important effects. First, it ensures that\n",
    "all errors are positive. Second, it penalizes larger errors much more\n",
    "than smaller ones. As a result, MSE strongly discourages large deviations\n",
    "between predictions and targets.\n",
    "\n",
    "MSE is widely used because it is smooth and differentiable, making it\n",
    "well-suited for gradient-based optimization. However, its sensitivity\n",
    "to outliers can be a disadvantage when data contains extreme values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ff2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "y_true = np.array([10, 12, 14, 16])\n",
    "y_pred = np.array([9, 13, 15, 14])\n",
    "\n",
    "np.mean((y_true - y_pred) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b02f6",
   "metadata": {},
   "source": [
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "Mean Absolute Error computes the average of the absolute differences\n",
    "between predicted and actual values.\n",
    "\n",
    "MAE = (1/n) Σ |y − ŷ|\n",
    "\n",
    "Unlike MSE, MAE treats all errors equally, regardless of their magnitude.\n",
    "This makes MAE more robust to outliers and easier to interpret, as the\n",
    "loss is expressed in the same units as the target variable.\n",
    "\n",
    "However, MAE is not differentiable at zero, which can make optimization\n",
    "slightly more challenging for some algorithms. Despite this, MAE is\n",
    "often preferred when robustness is more important than sensitivity\n",
    "to large errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ea4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.mean(np.abs(y_true - y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9eadfc",
   "metadata": {},
   "source": [
    "\n",
    "## Classification Loss\n",
    "\n",
    "Classification tasks involve predicting discrete class labels.\n",
    "Instead of measuring numerical distance, classification losses\n",
    "measure how well predicted probabilities align with true labels.\n",
    "\n",
    "These losses evaluate not only whether a prediction is correct,\n",
    "but also how confident the model is in its prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c668d27",
   "metadata": {},
   "source": [
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "Cross-entropy loss measures the difference between the true class\n",
    "distribution and the predicted probability distribution.\n",
    "\n",
    "For binary classification, it is defined as:\n",
    "\n",
    "L = −[y log(ŷ) + (1 − y) log(1 − ŷ)]\n",
    "\n",
    "Cross-entropy heavily penalizes confident but incorrect predictions.\n",
    "If the model assigns high probability to the wrong class, the loss\n",
    "becomes very large. This encourages the model to produce well-calibrated\n",
    "probabilities rather than just correct class labels.\n",
    "\n",
    "Cross-entropy is derived from probability theory and maximum likelihood\n",
    "estimation, making it theoretically well-founded and widely adopted\n",
    "in classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = np.array([1, 0, 1])\n",
    "y_pred_prob = np.array([0.9, 0.1, 0.6])\n",
    "\n",
    "- np.mean(\n",
    "    y_true * np.log(y_pred_prob) +\n",
    "    (1 - y_true) * np.log(1 - y_pred_prob)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef13b9",
   "metadata": {},
   "source": [
    "\n",
    "## Objective of Optimization\n",
    "\n",
    "The objective of optimization is to find model parameters that minimize\n",
    "the loss function. Optimization transforms learning into a mathematical\n",
    "problem of finding the minimum of a function.\n",
    "\n",
    "In neural networks, optimization operates in a high-dimensional parameter\n",
    "space, often containing millions of weights and biases. The optimization\n",
    "process searches this space iteratively, improving parameters step by step.\n",
    "\n",
    "A good optimization strategy balances:\n",
    "- Speed of convergence\n",
    "- Stability of updates\n",
    "- Ability to escape poor local solutions\n",
    "\n",
    "The effectiveness of a learning algorithm depends heavily on how well\n",
    "this optimization objective is defined and solved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09805249",
   "metadata": {},
   "source": [
    "\n",
    "## Gradient Descent Intuition\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm used to minimize\n",
    "the loss function. It works by computing the gradient (slope) of the loss\n",
    "with respect to model parameters and updating parameters in the opposite\n",
    "direction of the gradient.\n",
    "\n",
    "Intuitively, gradient descent can be visualized as moving downhill on\n",
    "a surface defined by the loss function. Each step moves the parameters\n",
    "closer to a minimum.\n",
    "\n",
    "The size and direction of each step are determined by:\n",
    "- The gradient\n",
    "- The learning rate\n",
    "\n",
    "Gradient descent forms the backbone of neural network training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2abdaa",
   "metadata": {},
   "source": [
    "\n",
    "## Conceptual Illustration: Loss Minimization\n",
    "\n",
    "```\n",
    "Loss\n",
    " ^\n",
    " |        *\n",
    " |      *\n",
    " |    *\n",
    " |  *\n",
    " |*\n",
    " +------------------> Model Parameters\n",
    "```\n",
    "\n",
    "The objective of training is to move the model parameters toward\n",
    "the region where the loss value is minimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d8935",
   "metadata": {},
   "source": [
    "\n",
    "## Conceptual Illustration: Gradient Descent Movement\n",
    "\n",
    "```\n",
    "Start  --->  --->  --->  Minimum\n",
    "  *      *      *      *\n",
    "```\n",
    "\n",
    "Each step taken by gradient descent moves the parameters closer\n",
    "to a minimum of the loss surface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a248d",
   "metadata": {},
   "source": [
    "\n",
    "## Confusion Matrix (Conceptual View)\n",
    "\n",
    "```\n",
    "                Predicted\n",
    "              Positive  Negative\n",
    "Actual Positive    TP        FN\n",
    "Actual Negative    FP        TN\n",
    "```\n",
    "\n",
    "Evaluation metrics such as precision, recall, and F1-score are\n",
    "derived from these four values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c79b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interactive example: effect of learning rate on convergence (conceptual)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def learning_rate_effect(lr):\n",
    "    print(f\"Learning Rate selected: {lr}\")\n",
    "    if lr < 0.01:\n",
    "        print(\"Slow convergence, very stable updates\")\n",
    "    elif lr < 0.1:\n",
    "        print(\"Good balance between speed and stability\")\n",
    "    else:\n",
    "        print(\"Risk of divergence or unstable training\")\n",
    "\n",
    "slider = widgets.FloatSlider(\n",
    "    value=0.05,\n",
    "    min=0.001,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    description='Learning Rate',\n",
    ")\n",
    "\n",
    "display(slider)\n",
    "widgets.interactive_output(learning_rate_effect, {'lr': slider})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a595bf",
   "metadata": {},
   "source": [
    "\n",
    "##  Types of Gradient Descent\n",
    "\n",
    "Gradient descent algorithms differ mainly in **how much data is used**\n",
    "to compute the gradient at each update step. This choice strongly affects\n",
    "training speed, stability, and memory usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148d77f5",
   "metadata": {},
   "source": [
    "\n",
    "### Batch Gradient Descent ()\n",
    "\n",
    "Batch Gradient Descent computes the gradient of the loss function using\n",
    "the **entire training dataset** before updating model parameters.\n",
    "\n",
    "Characteristics:\n",
    "- Produces smooth and stable updates\n",
    "- Guarantees convergence for convex problems\n",
    "- Computationally expensive for large datasets\n",
    "- Requires loading all data into memory\n",
    "\n",
    "Batch gradient descent is mainly used for:\n",
    "- Small datasets\n",
    "- Analytical studies\n",
    "- Convex optimization problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019bcc8",
   "metadata": {},
   "source": [
    "\n",
    "### Stochastic Gradient Descent ()\n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates model parameters using\n",
    "**a single training example at a time**.\n",
    "\n",
    "Characteristics:\n",
    "- Extremely fast updates\n",
    "- Noisy parameter updates\n",
    "- Helps escape shallow local minima\n",
    "- Lower memory requirements\n",
    "\n",
    "SGD introduces randomness into training, which often improves\n",
    "generalization performance despite noisy updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84518d6f",
   "metadata": {},
   "source": [
    "\n",
    "### Mini-batch Gradient Descent ()\n",
    "\n",
    "Mini-batch Gradient Descent is a compromise between batch and stochastic methods.\n",
    "It computes gradients using a **small subset (batch) of data**.\n",
    "\n",
    "Characteristics:\n",
    "- Efficient computation using vectorization\n",
    "- More stable than SGD\n",
    "- Faster than batch gradient descent\n",
    "- Most widely used in practice\n",
    "\n",
    "Typical mini-batch sizes range from 16 to 256 samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11351e24",
   "metadata": {},
   "source": [
    "\n",
    "##  Learning Rate and Its Effect\n",
    "\n",
    "The learning rate determines how far model parameters move during each\n",
    "gradient descent update. It directly controls the speed and stability\n",
    "of the learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a7265",
   "metadata": {},
   "source": [
    "\n",
    "### Effects of Different Learning Rates\n",
    "\n",
    "- **Very small learning rate**  \n",
    "  Slow convergence, long training time\n",
    "\n",
    "- **Very large learning rate**  \n",
    "  Overshooting minima, unstable training\n",
    "\n",
    "- **Well-chosen learning rate**  \n",
    "  Fast convergence and stable learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccfe03",
   "metadata": {},
   "source": [
    "\n",
    "Learning rate selection is often performed using:\n",
    "- Learning rate schedules\n",
    "- Adaptive optimizers\n",
    "- Empirical experimentation\n",
    "\n",
    "In practice, learning rate is one of the most critical hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848661d3",
   "metadata": {},
   "source": [
    "\n",
    "##  High-level Intuition of Backpropagation\n",
    "\n",
    "Backpropagation is the algorithm that enables neural networks to learn.\n",
    "It efficiently computes how each parameter contributes to the final loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c632a",
   "metadata": {},
   "source": [
    "\n",
    "### Error Signal Propagation\n",
    "\n",
    "- Output layer computes prediction error\n",
    "- Error is propagated backward layer by layer\n",
    "- Each layer receives a portion of the error\n",
    "- Parameters are updated using gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c44c3",
   "metadata": {},
   "source": [
    "\n",
    "Backpropagation relies on the **chain rule of calculus**, allowing gradients\n",
    "to be computed efficiently even in very deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8eec6",
   "metadata": {},
   "source": [
    "\n",
    "##  Evaluation Metrics\n",
    "\n",
    "Evaluation metrics provide insight into model performance beyond loss values.\n",
    "They are especially important when class distributions are uneven.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77f65a",
   "metadata": {},
   "source": [
    "\n",
    "### Accuracy ()\n",
    "\n",
    "Accuracy measures the proportion of correct predictions.\n",
    "\n",
    "Strengths:\n",
    "- Easy to interpret\n",
    "- Useful for balanced datasets\n",
    "\n",
    "Limitations:\n",
    "- Misleading for imbalanced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68519962",
   "metadata": {},
   "source": [
    "\n",
    "### Precision ()\n",
    "\n",
    "Precision measures how many predicted positive samples are actually positive.\n",
    "\n",
    "Precision is important when:\n",
    "- False positives are costly\n",
    "- Quality of positive predictions matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dfe809",
   "metadata": {},
   "source": [
    "\n",
    "### Recall ()\n",
    "\n",
    "Recall measures how many actual positive samples are correctly identified.\n",
    "\n",
    "Recall is important when:\n",
    "- Missing positive cases is costly\n",
    "- Detection coverage matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be0131",
   "metadata": {},
   "source": [
    "\n",
    "### F1-score ()\n",
    "\n",
    "F1-score balances precision and recall using their harmonic mean.\n",
    "\n",
    "F1-score is useful when:\n",
    "- Data is imbalanced\n",
    "- Both false positives and false negatives matter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b661d8",
   "metadata": {},
   "source": [
    "## Context and Motivation\n",
    "\n",
    "By this stage in our study, we have seen how data flows through a neural network, how tensors\n",
    "represent this data, and how activation functions introduce non-linearity into the model.\n",
    "We have also worked through a simple classification example and examined how real-world data\n",
    "must be prepared before it can be used for learning.\n",
    "\n",
    "With these components in place, a neural network is capable of producing predictions. However,\n",
    "prediction alone does not imply learning. A learning system must be able to evaluate its own\n",
    "performance and adjust itself accordingly. This requirement motivates the introduction of\n",
    "loss functions and evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc6e05",
   "metadata": {},
   "source": [
    "## Learning as Feedback\n",
    "\n",
    "Learning in neural networks relies on feedback. After the network produces an output, this\n",
    "output must be compared with the true target. The result of this comparison provides a signal\n",
    "that indicates how the model should change.\n",
    "\n",
    "Loss functions formalize this idea of feedback by converting prediction errors into numerical\n",
    "values. These values guide the optimization process that updates the network’s parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cadb3",
   "metadata": {},
   "source": [
    "## Why Different Tasks Need Different Loss Functions\n",
    "\n",
    "The nature of the prediction task strongly influences the choice of loss function. In\n",
    "regression problems, errors are measured as numerical deviations, while in classification\n",
    "problems, predictions are evaluated in terms of class membership and probability estimates.\n",
    "\n",
    "As a result, no single loss function is suitable for all tasks. The choice of loss function\n",
    "reflects assumptions about the data, the noise present in the observations, and the desired\n",
    "behavior of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20b85c",
   "metadata": {},
   "source": [
    "## Connecting Loss Functions to Optimization\n",
    "\n",
    "Loss functions are not chosen arbitrarily. In order to support efficient training, they must\n",
    "exhibit mathematical properties such as continuity and differentiability. These properties\n",
    "allow gradient-based optimization algorithms to compute meaningful parameter updates.\n",
    "\n",
    "This connection between loss functions and optimization explains why some intuitive performance\n",
    "measures are unsuitable as training objectives, even though they may be useful for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c9a8d6",
   "metadata": {},
   "source": [
    "## Interpreting Evaluation Metrics\n",
    "\n",
    "Evaluation metrics translate model performance into interpretable quantities. While loss\n",
    "functions operate internally during training, metrics are used to communicate results and to\n",
    "compare different models.\n",
    "\n",
    "Choosing appropriate evaluation metrics is especially important when dealing with imbalanced\n",
    "datasets or asymmetric costs of errors, where simple accuracy may provide a misleading picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f994de02",
   "metadata": {},
   "source": [
    "## Perspective\n",
    "\n",
    "Loss functions and evaluation metrics complete the learning pipeline introduced throughout the\n",
    "previous chapters. They connect model predictions back to data-driven objectives and provide\n",
    "the basis for systematic improvement.\n",
    "\n",
    "In later chapters, these concepts will be revisited in the context of more advanced models and\n",
    "training strategies, reinforcing their foundational role in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b470cb",
   "metadata": {},
   "source": [
    "\n",
    "## Task for the reader\n",
    "\n",
    "1. Compare MSE and MAE on datasets with outliers  \n",
    "2. Analyze the effect of learning rate changes  \n",
    "3. Explain why cross-entropy is preferred for classification  \n",
    "4. Compute evaluation metrics manually from predictions  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
